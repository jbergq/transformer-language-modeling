{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZflJMHzqm0zT"
      },
      "source": [
        "If you're opening this Notebook on colab, you will need to clone the repo and change directory. Uncomment the cell below and run it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXz94Ct2e8QG",
        "outputId": "003e8a1a-ffd6-4058-fd3a-c18e4fa00cb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'transformer' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jbergq/transformer.git && cd transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1msvJlmNm0zW",
        "outputId": "5697b3e4-d29b-4b0d-ece3-fb8730904bba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install portalocker\n",
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHT4HP5nkhkV"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfasgDSie4KW"
      },
      "source": [
        "## Transformer decoder implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section shows how to implement a transformer decoder, the version of the transformer suitable for language modeling."
      ],
      "metadata": {
        "id": "55A2oazulwPR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0UCKKRUkhkV"
      },
      "source": [
        "### Token embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In NLP, text is represented by sub-word units called tokens. Tokens can be thought of as a numeric representation of the smallest meaningful units of language. After tokenizing our training corpus, the text will be represented as sequences of integers that are easier for a machine learning model to work with.\n",
        "\n",
        "In the transformer, tokens are mapped to trainable vector representations called embeddings. Once trained, these embeddings represent various features of the sub-words they correspond to.\n",
        "\n",
        "Let's implement the token embedding using PyTorch's built-in `nn.Embedding` module."
      ],
      "metadata": {
        "id": "ZQnboIrMmArh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hb59wYjAkhkV"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.embedding(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So is that it? Not quite. While we could train our transformer with only token embeddings, the transformer architecture itself has no notion of the ordering of tokens. In natural language, the order of words can completely change the meaning of a sentence, so it is necessary to give our transformer a way to represent this. A common way to address this is to create another embedding that is simply added onto the token embeddings to inject information about its position, called a *positional encoding*.\n",
        "\n",
        "Let's see how it can be implemented. For a sequence length of $T$, we want to generate $T$ vectors that can uniquely represent each position in the sequence. We also want TODO."
      ],
      "metadata": {
        "id": "H8bex-bknXCC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOsSIkH0khkW"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, context_size, embedding_size, n=10000):\n",
        "        super().__init__()\n",
        "\n",
        "        i = torch.arange(embedding_size // 2)\n",
        "        k = torch.arange(context_size).unsqueeze(dim=1)\n",
        "\n",
        "        pos_embeddings = torch.zeros(context_size, embedding_size, requires_grad=False)\n",
        "        pos_embeddings[:, 0::2] = torch.sin(k / (n ** (2 * i / embedding_size)))\n",
        "        pos_embeddings[:, 1::2] = torch.cos(k / (n ** (2 * i / embedding_size)))\n",
        "\n",
        "        self.register_buffer(\"pos_embeddings\", pos_embeddings)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pos_embeddings[: x.shape[1], :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA3m27jakhkX"
      },
      "source": [
        "### Transformer blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers, like many other neural networks, are made by stacking multiple computational blocks in a sequence. Transformer blocks contain two main components:\n",
        "1. A multi-head attention module, responsible for communication between token embeddings.\n",
        "2. A feedforward module, responsible for processing token embeddings.\n",
        "\n",
        "By alternating between inter-token communication and per-token processing, transformers are able to produce sophisticated representations of language."
      ],
      "metadata": {
        "id": "JNObn9OMVgiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multi-head attention"
      ],
      "metadata": {
        "id": "0nSlS4dabVU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention is perhaps the most known and important component of transformers. TODO."
      ],
      "metadata": {
        "id": "cXSgiVOobbkS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7NMQVIpkhkX"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size, head, seq_length, head_size = k.shape\n",
        "\n",
        "        score = (q @ k.transpose(2, 3)) / math.sqrt(head_size)\n",
        "\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        score = self.softmax(score)\n",
        "\n",
        "        return score @ v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ69HG2dkhkY"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size, num_heads=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.attention = ScaledDotProductAttention()\n",
        "\n",
        "        self.lin_q = nn.Linear(embedding_size, hidden_size)\n",
        "        self.lin_k = nn.Linear(embedding_size, hidden_size)\n",
        "        self.lin_v = nn.Linear(embedding_size, hidden_size)\n",
        "\n",
        "        self.lin_concat = nn.Linear(hidden_size, embedding_size)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        q, k, v = self.lin_q(q), self.lin_k(k), self.lin_v(v)\n",
        "\n",
        "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
        "\n",
        "        out = self.attention(q, k, v, mask)\n",
        "\n",
        "        out = self.concat(out)\n",
        "        out = self.lin_concat(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def split(self, x):\n",
        "        batch_size, seq_len, hidden_size = x.shape\n",
        "\n",
        "        per_head_size = hidden_size // self.num_heads\n",
        "\n",
        "        return x.view(batch_size, seq_len, self.num_heads, per_head_size).transpose(\n",
        "            1, 2\n",
        "        )\n",
        "\n",
        "    def concat(self, x):\n",
        "        batch_size, num_heads, seq_len, head_size = x.shape\n",
        "        hidden_size = num_heads * head_size\n",
        "\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-fFUhnZkhkZ"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, in_size, hidden_size, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin1 = nn.Linear(in_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.lin2 = nn.Linear(hidden_size, in_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.lin2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM-BqKAVkhkZ"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        ff_hidden_size,\n",
        "        num_heads,\n",
        "        use_cross_attn,\n",
        "        dropout_prob=0.1,\n",
        "        layer_norm_eps=1e-5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention1 = MultiHeadAttention(hidden_size, hidden_size, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
        "        self.dropout1 = nn.Dropout(dropout_prob)\n",
        "\n",
        "        if use_cross_attn:\n",
        "            self.enc_dec_attention = MultiHeadAttention(\n",
        "                hidden_size, hidden_size, num_heads\n",
        "            )\n",
        "            self.enc_norm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
        "            self.norm2 = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
        "            self.dropout2 = nn.Dropout(dropout_prob)\n",
        "\n",
        "        self.ff_block = FeedForward(hidden_size, ff_hidden_size, dropout_prob)\n",
        "\n",
        "        self.norm3 = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
        "        self.dropout3 = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x, lookahead_mask=None):\n",
        "        x_n = self.norm1(x)\n",
        "        x_a = self.attention1(q=x_n, k=x_n, v=x_n, mask=lookahead_mask)\n",
        "\n",
        "        x = self.dropout1(x + x_a)\n",
        "\n",
        "        x_f = self.ff_block(self.norm3(x))\n",
        "\n",
        "        x = self.dropout3(x + x_f)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Putting it all together"
      ],
      "metadata": {
        "id": "L462eklccHQp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maMrEmGakhkZ"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        context_size,\n",
        "        hidden_size,\n",
        "        ff_hidden_size,\n",
        "        num_blocks=5,\n",
        "        num_heads=8,\n",
        "        use_cross_attn=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, hidden_size)\n",
        "        self.pos_embedding = PositionalEncoding(context_size, hidden_size)\n",
        "\n",
        "        self.decoder = []\n",
        "        for _ in range(num_blocks):\n",
        "            self.decoder.append(\n",
        "                DecoderBlock(\n",
        "                    hidden_size,\n",
        "                    ff_hidden_size,\n",
        "                    num_heads,\n",
        "                    use_cross_attn,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.decoder = nn.ModuleList(self.decoder)\n",
        "\n",
        "        self.ln_final = nn.LayerNorm(hidden_size)\n",
        "        self.lin_final = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, lookahead_mask=None):\n",
        "        x = self.token_embedding(tokens) + self.pos_embedding(tokens)\n",
        "\n",
        "        for block in self.decoder:\n",
        "            x = block(x, lookahead_mask)\n",
        "\n",
        "        out = self.lin_final(self.ln_final(x))\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQPEuCL4khkZ"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, context_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.context_size = context_size\n",
        "        self.register_buffer(\"tri\", torch.tril(torch.ones(context_size, context_size)))\n",
        "\n",
        "    def create_lookahead_mask(self, tgt_seq_len):\n",
        "        return self.tri[:tgt_seq_len, :tgt_seq_len].unsqueeze(0)\n",
        "\n",
        "\n",
        "class TransformerDecoder(Transformer):\n",
        "    \"\"\"Transformer decoder, using auto-regressive decoder blocks for language modeling.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        context_size,\n",
        "        hidden_size,\n",
        "        ff_hidden_size,\n",
        "        num_blocks=5,\n",
        "        num_heads=8,\n",
        "    ):\n",
        "        super().__init__(context_size)\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            vocab_size,\n",
        "            context_size,\n",
        "            hidden_size,\n",
        "            ff_hidden_size,\n",
        "            num_blocks,\n",
        "            num_heads,\n",
        "            use_cross_attn=False,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        lookahead_mask = self.create_lookahead_mask(x.shape[1])\n",
        "\n",
        "        out = self.decoder(x, None, lookahead_mask)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R37B9vSskhka"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19dc6w9skhka"
      },
      "source": [
        "### Util functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bij47miWkhka"
      },
      "outputs": [],
      "source": [
        "def count_model_params(model):\n",
        "    total_params = 0\n",
        "    for params in list(model.parameters()):\n",
        "        num = 1\n",
        "        for size in list(params.size()):\n",
        "            num = num * size\n",
        "        total_params += num\n",
        "    return total_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r2ZwIPckhka"
      },
      "outputs": [],
      "source": [
        "def train_start_print(model):\n",
        "    e_string = \"=\" * 45\n",
        "\n",
        "    num_params = count_model_params(model)\n",
        "    num_params_m = num_params / 1e6\n",
        "\n",
        "    print(\"\\n\" + e_string)\n",
        "    print(\"Starting training!\")\n",
        "    print(\"Num model params: {num_params_m:.3f}M\".format(num_params_m=num_params_m))\n",
        "    print(e_string + \"\\n\")\n",
        "\n",
        "\n",
        "def iter_print(iter, train_loss, newline_interval=50):\n",
        "    l_string = \"-\" * 45\n",
        "    f_str = \"{: <10} {: <10.5}\"\n",
        "\n",
        "    if iter % 500 == 0:\n",
        "        print(l_string)\n",
        "        print(f_str.format(\"Iter\", \"Train loss\"))\n",
        "        print(l_string)\n",
        "    print(f_str.format(iter, train_loss), end=\"\\r\" if iter % newline_interval else \"\\n\")\n",
        "\n",
        "\n",
        "def evaluation_print(losses):\n",
        "    e_string = \"=\" * 45\n",
        "\n",
        "    print(\"\\n\\n\" + e_string)\n",
        "    print(\"Evaluation done!\")\n",
        "    print(\"Mean train loss: {mean_loss:.3f}\".format(mean_loss=losses[\"train\"]))\n",
        "    print(\"Mean validation loss: {mean_loss:.3f}\".format(mean_loss=losses[\"val\"]))\n",
        "    print(e_string + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hk99apE1ewqq",
        "outputId": "8ec7b2c6-b6d0-48e3-c483-1cb3589f26c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'val_size': 1000, 'max_iters': 600000, 'eval_iters': 100, 'eval_interval': 1000, 'effective_batch_size': 512, 'batch_size': 4, 'grad_accum_steps': 128, 'lr': 0.001, 'warmup_iters': 2000, 'lr_decay_iters': 600000, 'min_lr': 6e-05, 'weight_decay': 0.0005, 'print_example': True}\n"
          ]
        }
      ],
      "source": [
        "from easydict import EasyDict\n",
        "\n",
        "\n",
        "# Define base config. Partly adopted from nanoGPT by Andrej Karpathy\n",
        "cfg = EasyDict(\n",
        "    {\n",
        "        \"val_size\": 1000,  # Size of validation set.\n",
        "        \"max_iters\": 600000,  # Total num training iterations.\n",
        "        \"eval_iters\": 100,  # Number of evaluation iterations.\n",
        "        \"eval_interval\": 1000,\n",
        "        \"effective_batch_size\": 512,\n",
        "        \"batch_size\": 4,\n",
        "        \"grad_accum_steps\": 1,\n",
        "        \"lr\": 1e-3,\n",
        "        \"warmup_iters\": 2000,\n",
        "        \"lr_decay_iters\": 600000,  # Should be ~= max_iters per Chinchilla.\n",
        "        \"min_lr\": 6e-5,  # Minimum learning rate, should be ~= learning_rate/10 per Chinchilla.\n",
        "        \"weight_decay\": 0.0005,\n",
        "        \"print_example\": True,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Derive accumulation steps to get target effective batch size.\n",
        "if cfg.effective_batch_size is not None:\n",
        "    cfg[\"grad_accum_steps\"] = cfg[\"effective_batch_size\"] // cfg[\"batch_size\"]\n",
        "\n",
        "print(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVpo2t6O3QHQ"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgmW-yU4ewqq"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "wandb.init(project=\"transformer\", config=cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTV1WsUcewqr"
      },
      "source": [
        "Let's setup our dataset. We will use Hugging Face's `datasets` package to prepare and load the WebText dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhxVrtT6ewqr"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load WebText dataset in streaming mode. No need to download!\n",
        "dataset = load_dataset(\"openwebtext\", streaming=True)[\"train\"]\n",
        "shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10000)\n",
        "\n",
        "# Split dataset.\n",
        "train_set = shuffled_dataset.skip(cfg.val_size)\n",
        "val_set = shuffled_dataset.take(cfg.val_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq0A_X30ewqr"
      },
      "source": [
        "To tokenize our dataset, we will use the GPT-2 tokenizer, available from Hugging Face's `transformers` package.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwR54R_Bewqs"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Tokenizer used by GPT-2.\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6OPgJhYewqs"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelWithLMHead, AutoConfig\n",
        "\n",
        "\n",
        "class GPT2Wrapped(nn.Module):\n",
        "    def __init__(self, pretrained=False) -> None:\n",
        "        super().__init__()\n",
        "        if pretrained:\n",
        "            self.model = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
        "        else:\n",
        "            config = AutoConfig.from_pretrained(\"gpt2\")\n",
        "            self.model = AutoModelWithLMHead.from_config(config)\n",
        "\n",
        "        self.context_size = self.model.config.n_ctx\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).logits\n",
        "\n",
        "\n",
        "def get_model(name, vocab_size):\n",
        "    if name == \"toy-model\":\n",
        "        return TransformerDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            context_size=64,\n",
        "            hidden_size=128,\n",
        "            ff_hidden_size=256,\n",
        "            num_blocks=4,\n",
        "            num_heads=4,\n",
        "        )\n",
        "    elif name == \"gpt2-small-custom\":\n",
        "        return TransformerDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            context_size=1024,\n",
        "            hidden_size=768,\n",
        "            ff_hidden_size=3072,\n",
        "            num_blocks=12,\n",
        "            num_heads=12,\n",
        "        )\n",
        "    elif name == \"gpt2-small-hf\":\n",
        "        return GPT2Wrapped(pretrained=False)\n",
        "    elif name == \"gpt2-small-hf-pretrained\":\n",
        "        return GPT2Wrapped(pretrained=True)\n",
        "\n",
        "\n",
        "# Edit below to select a model.\n",
        "model_name = \"toy-model\"\n",
        "\n",
        "model = get_model(model_name, tokenizer.vocab_size)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNJeQwyrewqs"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "\n",
        "def tokenize(example):\n",
        "    outputs = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,  # Truncate returned token sequences to max_length.\n",
        "        max_length=model.context_size + 1,  # Max length of returned token sequences.\n",
        "        return_overflowing_tokens=True,  # Tokenize whole input and split into chunks.\n",
        "        return_length=True,  # Return lengths of chunks.\n",
        "    )\n",
        "\n",
        "    # Create examples.\n",
        "    source_batch = []\n",
        "    target_batch = []\n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "        if length == model.context_size + 1:  # Only include full length sequences.\n",
        "            source_batch.append(input_ids[:-1])\n",
        "            target_batch.append(input_ids[1:])  # Note: Target is source shifted by one.\n",
        "\n",
        "    return {\"source\": source_batch, \"target\": target_batch}\n",
        "\n",
        "\n",
        "# Tokenize train and val sets.\n",
        "train_tokenized = train_set.map(\n",
        "    partial(tokenize),\n",
        "    batched=True,\n",
        "    remove_columns=train_set.column_names,\n",
        ")\n",
        "val_tokenized = val_set.map(\n",
        "    partial(tokenize),\n",
        "    batched=True,\n",
        "    remove_columns=val_set.column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3fudr_Fewqs"
      },
      "source": [
        "The training uses an \"infinite loop\" style, where we continue to sample random batches until we reach convergence or the maximum number of batches configured.\n",
        "\n",
        "Let's define a dataset wrapper that will allow us to continue sampling the dataset endlessly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibdFi9Ayewqt"
      },
      "outputs": [],
      "source": [
        "from typing import Iterator\n",
        "\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "\n",
        "class InfiniteIterableDataset(IterableDataset):\n",
        "    def __init__(self, hf_dataset, shuffle=False):\n",
        "        self.hf_dataset = hf_dataset\n",
        "\n",
        "    def __iter__(self) -> Iterator:\n",
        "        while True:\n",
        "            for item in self.hf_dataset:\n",
        "                yield item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcQYjs2Vewqt"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create data loaders for sampling batches.\n",
        "train_loader = DataLoader(\n",
        "    InfiniteIterableDataset(train_tokenized),\n",
        "    batch_size=cfg.batch_size,\n",
        "    collate_fn=lambda samples: {\n",
        "        \"source\": torch.tensor([sample[\"source\"] for sample in samples]),\n",
        "        \"target\": torch.tensor([sample[\"target\"] for sample in samples]),\n",
        "    },\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    InfiniteIterableDataset(val_tokenized),\n",
        "    batch_size=cfg.batch_size,\n",
        "    collate_fn=lambda samples: {\n",
        "        \"source\": torch.tensor([sample[\"source\"] for sample in samples]),\n",
        "        \"target\": torch.tensor([sample[\"target\"] for sample in samples]),\n",
        "    },\n",
        ")\n",
        "\n",
        "# Create data iterators.\n",
        "train_iter = iter(train_loader)\n",
        "val_iter = iter(train_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQqG7TX6ewqt"
      },
      "source": [
        "Let's load one train batch and one validation batch to make sure everything works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QneMUlKewqt"
      },
      "outputs": [],
      "source": [
        "batch_train = next(train_iter)\n",
        "\n",
        "print(batch_train[\"source\"][0][:10])\n",
        "print(batch_train[\"target\"][0][:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOSCMB3Gewqt"
      },
      "outputs": [],
      "source": [
        "batch_val = next(val_iter)\n",
        "\n",
        "print(batch_val[\"source\"][0][:10])\n",
        "print(batch_val[\"target\"][0][:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bFIHBBDm0zX"
      },
      "outputs": [],
      "source": [
        "def step(model, criterion, iterator):\n",
        "    batch = next(iterator)\n",
        "    src, tgt = batch[\"source\"].to(device), batch[\"target\"].to(device)\n",
        "\n",
        "    out = model(src)\n",
        "    # pred = out.softmax(dim=2).argmax(dim=2)\n",
        "\n",
        "    out_reshape = out.contiguous().view(-1, out.shape[-1])  # (B * T, vocab_size)\n",
        "    tgt_reshape = tgt.contiguous().view(-1)  # (B * T, 1)\n",
        "\n",
        "    loss = criterion(out_reshape, tgt_reshape)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKsUPNhDewqu"
      },
      "outputs": [],
      "source": [
        "# Loss estimation function inspired by nanoGPT repo by Andrej Karpathy.\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, criterion, train_iter, val_iter, eval_iters):\n",
        "    iterators = {\"train\": train_iter, \"val\": val_iter}\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split, iterator in iterators.items():\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            loss = step(model, criterion, iterator)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTz2YwUIewqu"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "\n",
        "# Learning rate decay scheduler inspired by nanoGPT repo by Andrej Karpathy.\n",
        "def get_lr(iter, warmup_iters, base_lr, min_lr, lr_decay_iters):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if iter < warmup_iters:\n",
        "        return base_lr * iter / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if iter > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (iter - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
        "    return min_lr + coeff * (base_lr - min_lr)\n",
        "\n",
        "\n",
        "get_lr_fn = partial(\n",
        "    get_lr,\n",
        "    warmup_iters=cfg.warmup_iters,\n",
        "    base_lr=cfg.lr,\n",
        "    min_lr=cfg.min_lr,\n",
        "    lr_decay_iters=cfg.lr_decay_iters,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggJLMBt7ewqu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "lrs = [get_lr_fn(iter) for iter in range(0, 1_000_000, 1)]\n",
        "plt.plot(lrs)\n",
        "plt.xlabel(\"iter\")\n",
        "plt.ylabel(\"lr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2NGkwNXewqu"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay, eps=5e-9)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEcsttYYewqu"
      },
      "source": [
        "To get an estimate for what loss values are reasonable to reach, let's load the original GPT-2 using HuggingFace and evaluate it with a few samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_L3vAwXewqu"
      },
      "outputs": [],
      "source": [
        "losses = estimate_loss(gpt2, criterion, train_iter, val_iter, 100)\n",
        "losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AiikKeFewqu"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, inp_seq, context_size, max_output_len=100):\n",
        "    seq = inp_seq\n",
        "\n",
        "    for _ in range(max_output_len):\n",
        "        out = model(seq[..., -context_size:])  # Truncate input sequence to max length.\n",
        "        probs = F.softmax(out[:, -1, :], dim=1)\n",
        "        next_tokens = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        # Append the next tokens to the generated sequences.\n",
        "        seq = torch.cat((seq, next_tokens), dim=-1)\n",
        "\n",
        "    return seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50nBJQLjm0zX"
      },
      "outputs": [],
      "source": [
        "fixed_inp = torch.tensor(\n",
        "    tokenizer.encode(\"The\"), dtype=torch.long, device=device\n",
        ").unsqueeze(0)\n",
        "\n",
        "if cfg.print_example:\n",
        "    batch = next(iter(train_loader))\n",
        "    out = generate(model, fixed_inp, cfg.context_size)\n",
        "\n",
        "    print(\"Example sequence: \", tokenizer.decode(batch[\"target\"][0].numpy())[:200])\n",
        "    print(\"Model output: \", tokenizer.decode(out[0].detach().cpu().numpy())[:200])\n",
        "\n",
        "# Reinitialize data iterators.\n",
        "train_iter = iter(train_loader)\n",
        "val_iter = iter(train_iter)\n",
        "\n",
        "iter_num = 0\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "model.train()\n",
        "\n",
        "# Start training.\n",
        "train_start_print(model)\n",
        "\n",
        "\n",
        "while True:\n",
        "    # Get learning rate according to schedule.\n",
        "    lr = get_lr_fn(iter_num)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "    # Train model on one batch.\n",
        "    train_loss = step(model, criterion, train_iter)\n",
        "    train_loss.backward()\n",
        "\n",
        "    # Accumulate gradients for N steps and update weights.\n",
        "    if (iter_num + 1) % cfg.grad_accum_steps == 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    if iter_num > 0 and iter_num % cfg.eval_interval == 0:\n",
        "        losses = estimate_loss(model, criterion, train_iter, val_iter, cfg.eval_iters)\n",
        "        evaluation_print(losses)\n",
        "\n",
        "        # Generate sample and print.\n",
        "        out = generate(model, fixed_inp, cfg.context_size)\n",
        "        print(\"Model output: \", tokenizer.decode(out[0].detach().cpu().numpy())[:200])\n",
        "\n",
        "        # Save model checkpoint if new best validation loss.\n",
        "        if losses[\"val\"] < best_val_loss:\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"iter\": iter_num,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                },\n",
        "                \"best.pth\",\n",
        "            )\n",
        "\n",
        "        # Log to WandB.\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"train/loss\": losses[\"train\"],\n",
        "                \"val/loss\": losses[\"val\"],\n",
        "                \"lr\": lr,\n",
        "            },\n",
        "            step=iter_num,\n",
        "        )\n",
        "\n",
        "    iter_print(iter_num, train_loss)\n",
        "    iter_num += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}