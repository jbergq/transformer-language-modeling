{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZflJMHzqm0zT"
      },
      "source": [
        "# Transformer Language Modeling\n",
        "\n",
        "This notebook shows how to implement and train a decoder-only transformer for language modeling. It was created to deepen my own understanding of transformers, but I hope it can also be a helpful resource for others wanting to learn more about this topic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NodcV6HC9KI8"
      },
      "source": [
        "## Getting started\n",
        "\n",
        "If you're opening this Notebook on colab, you will need to clone the repo and change working directory. Update the `do_clone` variable below to `True` and run the cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jXz94Ct2e8QG"
      },
      "outputs": [],
      "source": [
        "do_clone = False\n",
        "\n",
        "if do_clone:\n",
        "    !git clone https://github.com/jbergq/transformer.git\n",
        "    %cd transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1msvJlmNm0zW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install portalocker\n",
        "%pip install -r requirements.txt\n",
        "%pip install pyarrow\n",
        "%pip install --upgrade numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AHT4HP5nkhkV",
        "outputId": "6786c2fa-7fa3-4a25-c979-f3aedea994ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jonathanb/git/transformer/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W03kE9sZ9KI_"
      },
      "source": [
        "Now, let's create a configuration class that we will use to configure the remaining code in the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aiiw6wQD9KI_",
        "outputId": "ba54e5f0-4b47-4c45-a117-acacd6f93f14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'model_name': 'toy-model',\n",
              " 'val_size': 1000,\n",
              " 'max_iters': 600000,\n",
              " 'eval_iters': 100,\n",
              " 'eval_interval': 1000,\n",
              " 'effective_batch_size': 512,\n",
              " 'batch_size': 4,\n",
              " 'grad_accum_steps': 128,\n",
              " 'lr': 0.001,\n",
              " 'warmup_iters': 2000,\n",
              " 'lr_decay_iters': 600000,\n",
              " 'min_lr': 6e-05,\n",
              " 'weight_decay': 0.0005,\n",
              " 'print_example': True}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dataclasses import dataclass, asdict\n",
        "\n",
        "\n",
        "# Base config, partly adopted from nanoGPT by Andrej Karpathy\n",
        "@dataclass\n",
        "class Config:\n",
        "    model_name: str = \"toy-model\"\n",
        "    val_size: int = 1000  # Size of validation set.\n",
        "    max_iters: int = 600000  # Total num training iterations.\n",
        "    eval_iters: int = 100  # Number of evaluation iterations.\n",
        "    eval_interval: int = 1000\n",
        "    effective_batch_size: int = 512\n",
        "    batch_size: int = 4\n",
        "    grad_accum_steps: int = 1\n",
        "    lr: float = 1e-3\n",
        "    warmup_iters: int = 2000\n",
        "    lr_decay_iters: int = 600000  # Should be ~= max_iters per Chinchilla.\n",
        "    min_lr: float = 6e-5  # Minimum lr, should be ~= lr/10 per Chinchilla.\n",
        "    weight_decay: float = 0.0005\n",
        "    print_example: bool = True\n",
        "\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "# Derive accumulation steps to get target effective batch size.\n",
        "if cfg.effective_batch_size is not None:\n",
        "    cfg.grad_accum_steps = cfg.effective_batch_size // cfg.batch_size\n",
        "\n",
        "# Set model hparams.\n",
        "if cfg.model_name == \"toy-model\":\n",
        "    cfg.context_size = 64\n",
        "    cfg.embedding_size = 128\n",
        "    cfg.ff_hidden_size = 256\n",
        "    cfg.head_size = None\n",
        "    cfg.num_blocks = 4\n",
        "    cfg.num_heads = 4\n",
        "elif cfg.model_name == \"gpt2-small-custom\":\n",
        "    cfg.context_size = 1024\n",
        "    cfg.embedding_size = 768\n",
        "    cfg.ff_hidden_size = 3072\n",
        "    cfg.head_size = None\n",
        "    cfg.num_blocks = 12\n",
        "    cfg.num_heads = 12\n",
        "\n",
        "if cfg.head_size is None:\n",
        "    cfg.head_size = cfg.embedding_size // cfg.num_heads\n",
        "\n",
        "asdict(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfasgDSie4KW"
      },
      "source": [
        "## Model implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPzbFmV39KJD"
      },
      "source": [
        "This section shows how to implement a transformer decoder, the version of the transformer suitable for language modeling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Language modeling\n",
        "\n",
        "As the name implies, language modeling is the task of modeling natural language. Natural language is naturally represented as written text, or sequences of symbols, so the data used in this domain is therefore inherently sequential. Once trained on a large training set of text, often called _corpus_, a language model should learn to generate text that mimics the distribution of the training set. Mathematically, this means that a language model learns to model the probability distribution $p(x)$, where $x$ are varying length sequences of symbols $(s_1, ..., s_n)$.\n",
        "\n",
        "The probability distribution $p(x)$ can be factored into a product of conditional probabilities:\n",
        "\n",
        "\\begin{align*}\n",
        "p(x) = \\prod_{i=1}^n p(s_i | s_1, ..., s_{i-1})\n",
        "\\end{align*}\n",
        "\n",
        "Hence, if a language model can learn to model the conditional distributions on the right-hand side, $x \\sim p(x)$ can be sampled by iteratively sampling $s_i$ from the language model and appending them to its input. Language models like these are called _auto-regressive_.\n",
        "\n",
        "Thus, the task is to learn to model $p(s_i | s_1, ..., s_{n-i})$. The simplest way to translate this into a machine learning problem is to have the model predict a probability distribution over the entire vocabulary given $(s_1, ..., s_{i-1})$ as input, often called the _context_. For a training set of sequences $(x_1, ..., x_k)$, each consisting of sequences $(s_1, ..., s_n)$ of varying length $N$, we can form training pairs $(x, y)$ by extracting all sub-sequences on the form $\\big((s_1, ..., s_{i-1}), s_i \\big)$ for $2 \\leq i \\leq N$.\n",
        "\n",
        "Let's look at an example where the sequence is a normal sentence of words:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input                                                        Target\n",
            "--------------------------------------------------------------------------------\n",
            "Code                                                         morphs\n",
            "Code morphs                                                  into\n",
            "Code morphs into                                             wisdom\n",
            "Code morphs into wisdom                                      ,\n",
            "Code morphs into wisdom ,                                    bridging\n",
            "Code morphs into wisdom , bridging                           gaps\n",
            "Code morphs into wisdom , bridging gaps                      in\n",
            "Code morphs into wisdom , bridging gaps in                   understanding\n",
            "Code morphs into wisdom , bridging gaps in understanding     .\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "example_sentence = \"Code morphs into wisdom, bridging gaps in understanding.\"\n",
        "\n",
        "# Split into list of words and punctuation symbols.\n",
        "words = re.findall(r\"\\w+|[^\\w\\s]\", example_sentence)\n",
        "\n",
        "f_str = \"{: <60} {}\"\n",
        "print(f_str.format(\"Input\", \"Target\"))\n",
        "print(\"-\" * 80)\n",
        "for i in range(1, len(words)):\n",
        "    print(f_str.format(\" \".join(words[:i]), str(words[i])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The left column shows the input sequences $(s_1, ..., s_{i-1})$ and the right column the targets $s_i$.\n",
        "\n",
        "For most language models though, words in the form of text is not an suitable representation since arithmetic operations cannot be applied to it. We therefore need to convert the text into a numeric format, which we look into next.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokens\n",
        "\n",
        "Converting text into a numeric representation is typically called _tokenization_, and the elements of this representation are called _tokens_. In the simplest case, a text can be tokenized by splitting it into words and special symbols, just like the example above, and forming an ordered set called a _vocabulary_. When tokenizing a text, each word or symbol is replaced by its index in the vocabulary.\n",
        "\n",
        "Using words as tokens is a perfectly valid form of tokenization, but it has a drawback — it tends to lead to very large vocabularies (the Oxford English Dictionary contains about 170 000 words). As we will see later, the model's input and output layers scale with vocabulary size, so this tends to make its memory and compute footprint undesirably large.\n",
        "\n",
        "The other extreme would be to use the smallest units of written text as tokens, namely characters. For example, ASCII contains just 128 unique symbols and can represent most written text. Double the number of symbols to 256 and practically all text on the internet can be represented with this small vocabulary. The problem with this approach is that it becomes much more difficult to learn what any given symbol represents. A character alone can hardly be said to represent much, so this type tokenization scheme tends to be detrimental to the model's performance.\n",
        "\n",
        "The sweet spot in between these two is to use a scheme called _sub-word tokenization_, where words are split into parts which are tokenized. Of course, the question then becomes how to do the splitting, for which there are many approaches.\n",
        "\n",
        "TODO what _are_ tokens? How should they be understood in other modalities?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will be using the same tokenizer as GPT-2, which is a sub-word tokenizer based on a modified version of a technique called Byte Pair Encoding (BPE).\n",
        "\n",
        "Let's use Hugging Face's `transformers` package to load it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's tokenize the example sentence and decode it back to text to see how the sub-word tokenizer operates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized:  [10669, 17488, 82, 656, 11501, 11, 38265, 2667, 17332, 287, 4547, 13]\n",
            "Decoded:  ['Code', ' morph', 's', ' into', ' wisdom', ',', ' brid', 'ging', ' gaps', ' in', ' understanding', '.']\n"
          ]
        }
      ],
      "source": [
        "# GPT-2 used a subword tokenizer, meaning that each token corresponds to part of a word\n",
        "str_enc = tokenizer.encode(example_sentence)  # Tokenized string\n",
        "print(\"Tokenized: \", str_enc)\n",
        "print(\"Decoded: \", [tokenizer.decode([s]) for s in str_enc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that some words have indeed been split into sub-words, but others are kept intact. Most sub-word tokenizers aim to split words into meaningful units that can be composed together. For example, we see that the word \"morphs\" is split into \"morph\" and \"s\". Because of that, the model only has to learn the meaning of \"morph\" once, instead of separately for each of its grammatical variations. Additionally, it can now learn the general rule of how adding an \"s\" modifies the tense of the verb.\n",
        "\n",
        "In summary, tokenization is a crucial data preparation step for training language models. Furthermore, the transformer architecture is increasingly used for other data modalities such as vision and there is ongoing research on how to best perform tokenization for these domains. TODO finish.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4Tmse739KJD"
      },
      "source": [
        "### Transformer decoder\n",
        "\n",
        "In this section, we will be implementing a decoder-only transformer, the version of the transformer suitable for language modeling. The diagram below shows its architecture.\n",
        "\n",
        "<img src=\"https://github.com/jbergq/transformer/blob/main/images/transformer_decoder_preln.svg?raw=1\" height=720></img>\n",
        "\n",
        "We see that there are only a few constituent parts that make up the model architecture. These are:\n",
        "\n",
        "- A token embedding layer\n",
        "- $N$ transformer decoder blocks stacked sequentially\n",
        "- A classification head\n",
        "\n",
        "In the bottom of the image we also see the input sequence of tokens, denoted by $(s_1, s_2, s_3, ..., s_n)$. As described in the previous chapter, the transformer predicts the next token in the sequence, $s_{n+1}$, as a probability distribution over the vocabulary as a floating point vector, shown in the top of the image.\n",
        "\n",
        "Now, you may notice that there are multiple outputted probability vectors. The reason is this: While we _could_ train the transformer to only predict $s_{n+1}$, we would not be making the most out of our training data. Each sequence $(s_1, s_2, s_3, ..., s_n, s_{n+1})$ contains $n$ perfectly valid training examples $\\big((s_1), s_2\\big)$, $\\big((s_1, s_2), s_3\\big)$, $\\big((s_1, s_2, s_3), s_4\\big)$ and so on. We will be training the transformer to predict all of these _in parallel_.\n",
        "\n",
        "TODO expand on how transformers are suitable for parallelization.\n",
        "\n",
        "<!-- Therefore, given a training example sequence ${ s_1, s_2, s_3, ..., s_t, s_{t+1} }$, we will form an input sequence ${ s_1, s_2, ..., s_t }$ and a target sequence ${ s_2, s_3, ..., s_{t+1} }$.\n",
        "\n",
        "Here is a code example: -->\n",
        "\n",
        "Let's break down the transformer architecture and go over each layer separately, starting with the first — token embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0UCKKRUkhkV"
      },
      "source": [
        "### Token embeddings\n",
        "\n",
        "<img src=\"https://github.com/jbergq/transformer/blob/main/images/transformer_embedding.svg?raw=1\"></img>\n",
        "\n",
        "Simply put, the token embedding layer maps each token in the input sequence to learned vector representations called embeddings. Once trained, these embeddings can be thought of as representing various linguistic features of each token in the input, such as whether it is a verb or noun, singular or plural, etc. Tokens similar in meaning will likely share many such linguistic features, which means that their embeddings will be in close proximity in the embedding space. Embeddings therefore also provide a mathematical representation of the relatedness _between_ different tokens.\n",
        "\n",
        "Let's implement a simple token embedding using PyTorch's built-in `nn.Embedding` module. It is essentially a huge lookup table, with each row being the embedding vector for the token with that row number as value. The embeddings consist of parameters that can be trained via normal backpropagation. Since there are `vocab_size` tokens and each embedding has size `embedding_dim`, the embedding table has shape `(vocab_size, embedding_dim)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hb59wYjAkhkV"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.embedding(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8bex-bknXCC"
      },
      "source": [
        "So is that it? Not quite. While we could train our transformer with only token embeddings, the transformer architecture itself has no notion of the _positions_ of tokens in the sequence. In natural language, the order of words can completely change the meaning of a sentence, so it is important for our embeddings to reflect this information as well. A conventional way to address this for transformers is to create another embedding that is simply added onto the token embeddings to inject information about its position, called a _positional encoding_. Let's see how it can be implemented.\n",
        "\n",
        "#### Positional encoding\n",
        "\n",
        "For a sequence length of $N$, we want to generate $N$ vectors that can uniquely represent each position in the sequence. We also want TODO.\n",
        "\n",
        "TODO desirable properties etc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LOsSIkH0khkW"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, context_size, embedding_size, n=10000):\n",
        "        super().__init__()\n",
        "\n",
        "        i = torch.arange(embedding_size // 2)\n",
        "        k = torch.arange(context_size).unsqueeze(dim=1)\n",
        "\n",
        "        pos_embeddings = torch.zeros(context_size, embedding_size, requires_grad=False)\n",
        "        pos_embeddings[:, 0::2] = torch.sin(k / (n ** (2 * i / embedding_size)))\n",
        "        pos_embeddings[:, 1::2] = torch.cos(k / (n ** (2 * i / embedding_size)))\n",
        "\n",
        "        self.register_buffer(\"pos_embeddings\", pos_embeddings)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pos_embeddings[: x.shape[1], :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tvRTDGh9KJE"
      },
      "source": [
        "Let's create a single layer called a `TransformerEmbedding` that combines both of these embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_70_9kfu9KJE"
      },
      "outputs": [],
      "source": [
        "class TransformerEmbedding(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size, context_size) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, hidden_size)\n",
        "        self.pos_embedding = PositionalEncoding(context_size, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.token_embedding(x) + self.pos_embedding(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA3m27jakhkX"
      },
      "source": [
        "### Transformer blocks\n",
        "\n",
        "Next, we take a look at the decoder transformer block.\n",
        "\n",
        "<img src=\"https://github.com/jbergq/transformer/blob/main/images/transformer_block.svg?raw=1\"></img>\n",
        "\n",
        "Transformers, like many other neural networks, are made by stacking multiple computational blocks in a sequence. For decoder-type transformers, the blocks contain two main components:\n",
        "\n",
        "- A masked multi-head attention layer, applied across the whole sequence.\n",
        "- A feedforward block, applied to each token separately.\n",
        "\n",
        "By alternating between *inter-token communication* and *per-token processing*, transformers are able to produce sophisticated representations of language. Note that other than the attention layer, all layers in a transformer are applied to each token separately. It is only through attention that token-to-token interaction is considered.\n",
        "\n",
        "*Decoder* transformer blocks have a single property that defines them: they use _masked_ attention. This means that any given token only attends to the tokens that come before it, making the attention _causal_. More on this later.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nSlS4dabVU_"
      },
      "source": [
        "#### Attention mechanism\n",
        "\n",
        "Attention is arguably the most characteristic and important feature of transformers as it allows them to learn the contextual relationships *between* tokens. When attention is applied between tokens of the same sequence, it is called *self-attention*. For decoder transformers, this is the only form of attention used.\n",
        "\n",
        "There are multiple ways of implementing attention in neural networks, but the type commonly used in transformers is called _scaled dot product attention_. We will see how it works below.\n",
        "\n",
        "#### Summarizing the context\n",
        "\n",
        "First, let's look at a simplified example to build up intuition for why the full implementation takes the form it does. Recall that the transformer's objective is to predict the next token given the input sequence:\n",
        "\n",
        "\\begin{align*}\n",
        "p(s_{n+1} | s_{1}, ..., s_n)\n",
        "\\end{align*}\n",
        "\n",
        "To do this, the transformer needs a way to encode information about the input sequence $(s_{1}, ..., s_{n})$, called the _context_.\n",
        "\n",
        "<!--\n",
        "Note: Masked (causal) attention makes training and inference more efficient since you can train on all sub-examples in parallel and reuse embeddings during inference.\n",
        "You *could* have classification head only predict the next token given the context and not use masking. Would anything be gained from this?\n",
        "-->\n",
        "\n",
        "One way to summarize the context would be to simply take the mean of the token embeddings $e_1, e_2, ..., e_{i}$ and use the resulting vector to predict $p(s_{i+1} | s_{1}, ..., s_i)$.\n",
        "\n",
        "Remember that our transformer will predict the next token for *each* position $i$ in the input sequence, so we form one average per position $i$ on the form:\n",
        "\n",
        "\\begin{align}\n",
        "\\bar{e}_i = \\frac{1}{i} \\sum_j^i e_j\n",
        "\\end{align}\n",
        "\n",
        "<!-- TODO illustrative figure -->\n",
        "\n",
        "However, there are two problems with this approach:\n",
        "\n",
        "1. All elements of the sum are given the same \"weight\" $\\frac{1}{i}$ and therefore contribute equally to the result. In reality, some words may actually be more important than others for doing the prediction.\n",
        "2. The result is the same regardless of the order of $e_j$. This is problematic since meaning in language is highly dependent on the order of words.\n",
        "\n",
        "We will see that scaled dot product attention combined with the positional encodings explained earlier addresses both of these concerns.\n",
        "\n",
        "<!-- Point 3. means that we have to consider\n",
        "\n",
        "Why not have the model learn to *predict* the weights $w_j$?\n",
        "\n",
        "The way this is done in scaled dot product attention is as follows: -->\n",
        "\n",
        "#### Scaled dot product attention\n",
        "\n",
        "To improve upon the earlier example and better encode the context, we would like to have a way of computing the weighted sum\n",
        "\n",
        "\\begin{align*}\n",
        "\\bar{e}_i = \\sum_j^i w_{ij} e_j\n",
        "\\end{align*}\n",
        "\n",
        "with the weights $w_{ij}$ reflecting the \"importance\" of token $j$ for token $i$. Better yet, we would like the weights to also take the position $j$ in relation to the position $i$ into consideration.\n",
        "\n",
        "<!-- \n",
        "But what is even the importance of an embedding? Well, it likely depends on a variety of factors. Let's see if we can think of a few:\n",
        "\n",
        "1. The semantic meaning of the token. For example, TODO.\n",
        "2. The position of the token in the sentence. For example, the last word with position $t$ will likely be useful when predicting\n",
        "3. The position of the token in relation to other tokens and their positions. For example, TODO. -->\n",
        "\n",
        "To achieve this, we will design a mechanism for the model to learn to *predict* the weights $w_{ij}$. We will call $w_{ij}$ the *attention* or *attention score* between token $i$ and token $j$.\n",
        "\n",
        "The idea is as follows: For *each* token, we predict two embeddings, $q$ and $k$, called its $query$ and $key$. The weight $w_{ij}$ will then be formed by taking the dot product between the query of token $i$ and the key of token $j$:\n",
        "\n",
        "\\begin{align*}\n",
        "w_{ij} = q_i \\cdot k_j\n",
        "\\end{align*}\n",
        "\n",
        "Intuitively, queries can be thought of as representing what the token at position $i$ being summarized is \"looking for\", and the key represents what other tokens \"provide\".\n",
        "\n",
        "To predict $q$ and $k$, we introduce two simple linear mappings (weight matrices), $W_Q$ and $W_K$, which map each embedding $e_i$ to its key and query embeddings $q_i$ and $k_i$.\n",
        "\n",
        "\\begin{align*}\n",
        "q_i = W_Q e_i \\quad k_i = W_K e_i\n",
        "\\end{align*}\n",
        "\n",
        "Now, recall that at least in the initial decoder block, each embedding $e_i$ consists of two terms, namely the token embedding and positional encoding:\n",
        "\n",
        "\\begin{align*}\n",
        "e_i = t_i + p_i\n",
        "\\end{align*}\n",
        "\n",
        "Substituting all of this in equation TODO above yields\n",
        "\n",
        "\\begin{align*}\n",
        "w_{ij} &= q_i \\cdot k_j = W_Q e_i \\cdot W_K e_j = e_i^\\intercal W_Q^\\intercal W_K e_j = (t_i + p_j)^\\intercal W_Q^\\intercal W_K (t_j + p_j) \\\\\n",
        "&= \\underbrace{ t_i^\\intercal W_Q^\\intercal W_K t_j}_\\text{token i to token j} + \\underbrace{ t_i^\\intercal W_Q^\\intercal W_K p_j}_\\text{token i to position j} + \\underbrace{ p_i^\\intercal W_Q^\\intercal W_K t_j }_\\text{position i to token j} + \\underbrace{ t_i^\\intercal W_Q^\\intercal W_K t_j }_\\text{position i to position j}\n",
        "\\end{align*}\n",
        "\n",
        "Hence, the weights $w_{ij}$ will take into account both the relative semantic meaning and the relative positions between token pairs. The weight matrices $W_Q$ and $W_K$ are simply trained through normal gradient descent.\n",
        "\n",
        "Now, there are a few missing pieces before we have arrived at the full scaled dot product attention implementation. First, our notation becomes more concise if we swap to matrices instead. The weighted sum in equation TODO can be rewritten as the following matrix equation:\n",
        "\n",
        "\\begin{align*}\n",
        "\\bar{E} = W E = Q K^\\intercal E\n",
        "\\end{align*}\n",
        "\n",
        "with $W$ containing the weights $w_{ij}$ and $E, Q, K$ containing the embeddings $e_i$, queries $q_i$ and keys $k_i$ as columns.\n",
        "\n",
        "Further, the weights $w_{ij}$ are currently unbounded. To keep the interpretation as a weighted sum, we normalize them to sum to 1 by taking the softmax along each row.\n",
        "\n",
        "\\begin{align*}\n",
        "\\bar{E} = softmax(Q K^\\intercal) E\n",
        "\\end{align*}\n",
        "\n",
        "In practice, it has been found that the raw dot products $Q K^\\intercal$ can be spiky early in training, resulting in a non-uniform softmax distribution. This amounts to the model randomly starting out with strong attention towards particular tokens, taking time to unlearn and thus slowing down the training. This can be avoided by normalizing $Q K^\\intercal$ by its standard deviation, which can be worked out to be $\\sqrt{d_k}$, where $d_k$ is the embedding size.\n",
        "\n",
        "\\begin{align*}\n",
        "\\bar{E} = softmax(\\frac{Q K^\\intercal}{\\sqrt{d_k}} ) E\n",
        "\\end{align*}\n",
        "\n",
        "Finally, we have one missing piece: Instead of summing over the embeddings $e_i$ as in equation TODO, we introduce a third linear mapping $W_V$ to map them to embeddings $v_i$ called *values*. This is done for multiple reasons, among others:\n",
        "1. It allows the triplets $q_i, k_i, v_i$ to have a different embedding size compared to $e_i$\n",
        "2. Since $W_V$ now maps $e_i$ to $v_i$, it can learn to focus on particular semantic concepts to pass on through the sum TODO for summarizing the context.\n",
        "3. As we will see in a second, we will use *multiple* scaled dot product attention layers *in parallel*, each being a so-called *head*. Each head will have its own matrix triplets $W_Q, W_K, W_V$ and can therefore specialize on particular semantic concepts.    \n",
        "\n",
        "Replacing $E$ with $V = W_V E$ in the equation TODO gives us the final equation for the scaled dot product attention, as first presented in the original paper:\n",
        "\n",
        "\\begin{align*}\n",
        "Attention(Q, K, V) = softmax(\\frac{Q K^\\intercal}{\\sqrt{d_k}} ) V\n",
        "\\end{align*}\n",
        "\n",
        "TODO Lookahead masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All right, now let's see how we can implement this in code. We create a new module called `ScaledDotProductAttention` which will perform the operations in equation TODO with queries, keys and values as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "u7NMQVIpkhkX"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size, head, seq_length, head_size = k.shape\n",
        "\n",
        "        # Get attention scores through scaled dot product between queries and keys.\n",
        "        score = (q @ k.transpose(2, 3)) / math.sqrt(head_size)\n",
        "\n",
        "        # Prevent tokens attending to future tokens (decoder feature). Mask-fill these\n",
        "        # attention scores such that they are zeroed in softmax.\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Softmax attention scores to make them sum to 1.\n",
        "        score = self.softmax(score)\n",
        "\n",
        "        return score @ v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsJ665sL9KJF"
      },
      "source": [
        "Next, we will create another module called  `MultiHeadAttention`. All it does is simply to take the embeddings $e_i$ as input, compute the queries, keys and values *for each head* in parallel, perform the attention operation and pass the concatenated result through a linear layer to project it back down to the original embedding dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UZ69HG2dkhkY"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embedding_size, head_size, num_heads=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.head_size = head_size\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.attention = ScaledDotProductAttention()\n",
        "\n",
        "        self.lin_q = nn.Linear(embedding_size, head_size * num_heads)\n",
        "        self.lin_k = nn.Linear(embedding_size, head_size * num_heads)\n",
        "        self.lin_v = nn.Linear(embedding_size, head_size * num_heads)\n",
        "\n",
        "        self.lin_concat = nn.Linear(head_size * num_heads, embedding_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Get keys, queries and values for each head and token.\n",
        "        q, k, v = self.lin_q(x), self.lin_k(x), self.lin_v(x)\n",
        "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
        "\n",
        "        # Get attention-weighted token values for each head.\n",
        "        out = self.attention(q, k, v, mask)\n",
        "\n",
        "        # Concatenate head values and process for each token separately.\n",
        "        out = self.concat(out)\n",
        "        out = self.lin_concat(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def split(self, x):\n",
        "        batch_size, seq_len, hidden_size = x.shape\n",
        "\n",
        "        return x.view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
        "\n",
        "    def concat(self, x):\n",
        "        batch_size, num_heads, seq_len, head_size = x.shape\n",
        "        hidden_size = head_size * num_heads\n",
        "\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's it! We have implemented masked multi-head attention, the first out of two main componenters in the transformer block. Next, let's take a look at the second, the feedforward block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhzIGpZud2_W"
      },
      "source": [
        "#### Feedforward block\n",
        "\n",
        "We have now seen how the attention layer provides a mechanism for summarizing the input context by taking relationships between tokens into account. However, it does not inherently alter the representation of the input, so we also need a way to *process* the result. Typically, a simple feedforward block is used for this. It simply consists of two linear layers with non-linear activation functions, one for mapping the input into a higher-dimensional space and another for \"projecting\" it down to the original space again.\n",
        "\n",
        "We create a module called `FeedForward` for this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "f-fFUhnZkhkZ"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, in_size, hidden_size, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin1 = nn.Linear(in_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.lin2 = nn.Linear(hidden_size, in_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.lin2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let's create the `DecoderBlock` and put it all together. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wM-BqKAVkhkZ"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        ff_hidden_size,\n",
        "        head_size,\n",
        "        num_heads,\n",
        "        dropout_prob=0.1,\n",
        "        layer_norm_eps=1e-5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
        "        self.attention1 = MultiHeadAttention(hidden_size, head_size, num_heads)\n",
        "        self.dropout1 = nn.Dropout(dropout_prob)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
        "        self.ff_block = FeedForward(hidden_size, ff_hidden_size, dropout_prob)\n",
        "        self.dropout2 = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x, lookahead_mask=None):\n",
        "        # Pre-LN norm.\n",
        "        x_n = self.norm1(x)\n",
        "\n",
        "        # Inter-token communication via attention.\n",
        "        x_a = self.attention1(x_n, mask=lookahead_mask)\n",
        "\n",
        "        # Skip and dropout.\n",
        "        x = self.dropout1(x + x_a)\n",
        "\n",
        "        # Pre-LN norm.\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        # Per-token processing via feedforward block.\n",
        "        x_f = self.ff_block(x)\n",
        "\n",
        "        # Skip and dropout.\n",
        "        x = self.dropout2(x + x_f)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classification head\n",
        "\n",
        "The final layer to implement is the classification head. This simply takes the embeddings outputted by the final `DecoderBlock` and predicts the next token for each position in the input. Remember that each token prediction is made by forming a probability distribution over the vocabulary. We simply predict the logits with a linear layer that will go into a softmax outside of the model for the probabilities.\n",
        "\n",
        "Here it is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YsS8M8wv9KJT"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.ln_final = nn.LayerNorm(hidden_size)\n",
        "        self.lin_final = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Final pre-LN norm.\n",
        "        x = self.ln_final(x)\n",
        "\n",
        "        # Predict logits across vocabulary.\n",
        "        x = self.lin_final(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RspQlOJG9KJT"
      },
      "source": [
        "### Building the transformer\n",
        "\n",
        "Finally, we have all the components necessary for building our transformer decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "maMrEmGakhkZ"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        context_size,\n",
        "        embedding_size,\n",
        "        ff_hidden_size,\n",
        "        head_size,\n",
        "        num_blocks=5,\n",
        "        num_heads=8,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"tri\", torch.tril(torch.ones(context_size, context_size)))\n",
        "\n",
        "        self.embedding = TransformerEmbedding(embedding_size, vocab_size, context_size)\n",
        "\n",
        "        self.decoder = []\n",
        "        for _ in range(num_blocks):\n",
        "            self.decoder.append(DecoderBlock(embedding_size, ff_hidden_size, head_size, num_heads))\n",
        "        self.decoder = nn.ModuleList(self.decoder)\n",
        "\n",
        "        self.classifier = Classifier(embedding_size, vocab_size)\n",
        "\n",
        "    def create_lookahead_mask(self, tgt_seq_len):\n",
        "        return self.tri[:tgt_seq_len, :tgt_seq_len].unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Create decoder lookahead mask according to sequence length.\n",
        "        lookahead_mask = self.create_lookahead_mask(x.shape[1])\n",
        "\n",
        "        # Embed tokens.\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Process using decoder blocks.\n",
        "        for block in self.decoder:\n",
        "            x = block(x, lookahead_mask)\n",
        "\n",
        "        # Predict next tokens.\n",
        "        out = self.classifier(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTuoI3gi9KJA"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "We will now set up the training and validation datasets. We will use Hugging Face's `datasets` package to prepare and load the WebText dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZQFSR3Ca9KJA"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load WebText dataset in streaming mode. No need to download!\n",
        "dataset = load_dataset(\"openwebtext\", streaming=True)[\"train\"]\n",
        "shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10000)\n",
        "\n",
        "# Split dataset.\n",
        "train_set = shuffled_dataset.skip(cfg.val_size)\n",
        "val_set = shuffled_dataset.take(cfg.val_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTPRUCQB9KJB"
      },
      "source": [
        "Below we define a function called `tokenize` for turning text documents from the dataset into the example input and output sequences for training the model. We will be training a language model which simply predicts the next token in a sequence given the tokens so far as input. Therefore, input sequences will just be the target sequences shifted by one index to the left.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "B8ddb9cj9KJB"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "\n",
        "def tokenize(example):\n",
        "    outputs = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,  # Truncate returned token sequences to max_length.\n",
        "        max_length=cfg.context_size + 1,  # Max length of returned token sequences.\n",
        "        return_overflowing_tokens=True,  # Tokenize whole input and split into chunks.\n",
        "        return_length=True,  # Return lengths of chunks.\n",
        "    )\n",
        "\n",
        "    # Create examples.\n",
        "    inp_batch = []\n",
        "    tgt_batch = []\n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "        if length == cfg.context_size + 1:  # Only include full length sequences.\n",
        "            inp_batch.append(input_ids[:-1])\n",
        "            tgt_batch.append(input_ids[1:])  # Note: input shifted by one.\n",
        "\n",
        "    return {\"input\": inp_batch, \"target\": tgt_batch}\n",
        "\n",
        "\n",
        "# Tokenize train and val sets.\n",
        "train_tokenized = train_set.map(\n",
        "    partial(tokenize),\n",
        "    batched=True,\n",
        "    remove_columns=train_set.column_names,\n",
        ")\n",
        "val_tokenized = val_set.map(\n",
        "    partial(tokenize),\n",
        "    batched=True,\n",
        "    remove_columns=val_set.column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw6gSpct9KJC"
      },
      "source": [
        "We will use an \"infinite training loop\" to train the transformer language model, where we continue to sample random batches until we reach convergence or the maximum number of batches configured.\n",
        "\n",
        "Let's create a dataset wrapper that will allow us to continue sampling the dataset endlessly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "l1GEipZT9KJC"
      },
      "outputs": [],
      "source": [
        "from typing import Iterator\n",
        "\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "\n",
        "class InfiniteIterableDataset(IterableDataset):\n",
        "    def __init__(self, hf_dataset, shuffle=False):\n",
        "        self.hf_dataset = hf_dataset\n",
        "\n",
        "    def __iter__(self) -> Iterator:\n",
        "        while True:\n",
        "            for item in self.hf_dataset:\n",
        "                yield item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnIgZsGH9KJC"
      },
      "source": [
        "Finally, we create data loaders to sample batches from the datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4DzN8dMD9KJC"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    InfiniteIterableDataset(train_tokenized),\n",
        "    batch_size=cfg.batch_size,\n",
        "    collate_fn=lambda samples: {\n",
        "        \"input\": torch.tensor([sample[\"input\"] for sample in samples]),\n",
        "        \"target\": torch.tensor([sample[\"target\"] for sample in samples]),\n",
        "    },\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    InfiniteIterableDataset(val_tokenized),\n",
        "    batch_size=cfg.batch_size,\n",
        "    collate_fn=lambda samples: {\n",
        "        \"input\": torch.tensor([sample[\"input\"] for sample in samples]),\n",
        "        \"target\": torch.tensor([sample[\"target\"] for sample in samples]),\n",
        "    },\n",
        ")\n",
        "\n",
        "train_iter = iter(train_loader)\n",
        "val_iter = iter(train_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPg7bLmw9KJC"
      },
      "source": [
        "Load a train batch and a validation batch to make sure everything works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0THcSJpZ9KJC",
        "outputId": "3f53bf68-9a5a-44a4-8970-647e6ba06566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input sequence:  tensor([47976, 33265,    11,  5267,    13,   362,   357,    34,  4535,     8])\n",
            "Target sequence:  tensor([33265,    11,  5267,    13,   362,   357,    34,  4535,     8,  6185])\n"
          ]
        }
      ],
      "source": [
        "batch_train = next(train_iter)\n",
        "\n",
        "print(\"Input sequence: \", batch_train[\"input\"][0][:10])\n",
        "print(\"Target sequence: \", batch_train[\"target\"][0][:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "C2asNtwK9KJD",
        "outputId": "785a2728-9220-4af0-c50e-a4e04e946abd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input sequence:  tensor([12256, 35359, 23983,   968, 23222, 13072,  4482,   198,   198,   464])\n",
            "Target sequence:  tensor([35359, 23983,   968, 23222, 13072,  4482,   198,   198,   464,  1380])\n"
          ]
        }
      ],
      "source": [
        "batch_val = next(val_iter)\n",
        "\n",
        "print(\"Input sequence: \", batch_val[\"input\"][0][:10])\n",
        "print(\"Target sequence: \", batch_val[\"target\"][0][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq2Mcl2e9KJD"
      },
      "source": [
        "Great, it seems to work! Notice how the target sequences are just input sequences shifted one step to the left."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R37B9vSskhka"
      },
      "source": [
        "## Training\n",
        "\n",
        "At last, we are ready to setup the training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19dc6w9skhka"
      },
      "source": [
        "### Learning rate scheduling\n",
        "\n",
        "TODO describe lr schedule and Chinchilla details\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "v_wxV90YjnUq",
        "outputId": "4adf333c-d497-4298-ddb7-e89116faa66b"
      },
      "outputs": [],
      "source": [
        "# Learning rate decay scheduler inspired by nanoGPT repo by Andrej Karpathy.\n",
        "def get_lr(iter, warmup_iters, base_lr, min_lr, lr_decay_iters):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if iter < warmup_iters:\n",
        "        return base_lr * iter / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if iter > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (iter - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
        "    return min_lr + coeff * (base_lr - min_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'lr')"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "lrs = [\n",
        "    get_lr(\n",
        "        iter,\n",
        "        warmup_iters=cfg.warmup_iters,\n",
        "        base_lr=cfg.lr,\n",
        "        min_lr=cfg.min_lr,\n",
        "        lr_decay_iters=cfg.lr_decay_iters,\n",
        "    )\n",
        "    for iter in range(0, 1_000_000, 1)\n",
        "]\n",
        "plt.plot(lrs)\n",
        "plt.xlabel(\"iter\")\n",
        "plt.ylabel(\"lr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A6hdPR3h8sl"
      },
      "source": [
        "### Expected loss\n",
        "\n",
        "TODO scaling laws? Train ~= val loss?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Go7Vgqsi9KJV"
      },
      "outputs": [],
      "source": [
        "model = TransformerDecoder(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    context_size=cfg.context_size,\n",
        "    embedding_size=cfg.embedding_size,\n",
        "    ff_hidden_size=cfg.ff_hidden_size,\n",
        "    head_size=cfg.head_size,\n",
        "    num_blocks=cfg.num_blocks,\n",
        "    num_heads=cfg.num_heads,\n",
        ")\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay, eps=5e-9)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEcsttYYewqu"
      },
      "source": [
        "To get an estimate for what loss values are reasonable to reach, let's load the original GPT-2 using HuggingFace and evaluate it with a few samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss estimation function inspired by nanoGPT repo by Andrej Karpathy.\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, criterion, train_iter, val_iter, num_iters):\n",
        "    iterators = {\"train\": train_iter, \"val\": val_iter}\n",
        "    loss_dict = {}\n",
        "    model.eval()\n",
        "    for split, iterator in iterators.items():\n",
        "        losses = torch.zeros(num_iters)\n",
        "        for k in range(num_iters):\n",
        "            batch = next(iterator)\n",
        "            inp, tgt = batch[\"input\"].to(device), batch[\"target\"].to(device)\n",
        "\n",
        "            out = model(inp)\n",
        "\n",
        "            out_reshape = out.contiguous().view(-1, out.shape[-1])  # (B * T, vocab_size)\n",
        "            tgt_reshape = tgt.contiguous().view(-1)  # (B * T, 1)\n",
        "\n",
        "            loss = criterion(out_reshape, tgt_reshape)\n",
        "            losses[k] = loss.item()\n",
        "        loss_dict[split] = losses.mean()\n",
        "    model.train()\n",
        "\n",
        "    return loss_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "I_L3vAwXewqu",
        "outputId": "d22e5dc5-7322-41b2-d011-df968ba5a2d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jonathanb/git/transformer/venv/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:1322: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'train': tensor(3.5675), 'val': tensor(3.5477)}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelWithLMHead, AutoConfig\n",
        "\n",
        "\n",
        "class GPT2Wrapped(nn.Module):\n",
        "    def __init__(self, pretrained=False) -> None:\n",
        "        super().__init__()\n",
        "        if pretrained:\n",
        "            self.model = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
        "        else:\n",
        "            config = AutoConfig.from_pretrained(\"gpt2\")\n",
        "            self.model = AutoModelWithLMHead.from_config(config)\n",
        "\n",
        "        self.context_size = self.model.config.n_ctx\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).logits\n",
        "\n",
        "\n",
        "gpt2 = GPT2Wrapped(pretrained=True)\n",
        "\n",
        "losses = estimate_loss(gpt2, criterion, train_iter, val_iter, 100)\n",
        "losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Noygcooe9KJW"
      },
      "source": [
        "We see that the pre-trained GPT-2 reaches train and validation losses of about 3.5. Let's see how close we get!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "50nBJQLjm0zX",
        "outputId": "2084ea8b-c039-4fe8-8939-a1419df03e4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example sequence:  pei, Nov. 2 (CNA) Bitcoin is illegal in Taiwan, said Financial Supervisory Commission (FSC) Chairman Tseng Ming-chung (曾銘宗) on Monday after kidnappers in Taiwan tried to collect a ransom through the v\n",
            "Model output:  The Koen AVGVolume alum Adams Debor MUS crochet Knifecould Graph SPR unconstitutional Haas Pixar frameworkspat drum ICE Las mysterious Apocalypse Influence 58 riperofp didnt broadcaster domest Shareel\n",
            "\n",
            "=============================================\n",
            "Starting training!\n",
            "Num model params: 13.446M\n",
            "=============================================\n",
            "\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "0          11.027    \n",
            "50         11.01     \n",
            "100        10.972    \n",
            "150        10.98     \n",
            "200        10.981    \n",
            "250        11.018    \n",
            "300        10.873    \n",
            "350        10.938    \n",
            "400        10.907    \n",
            "450        10.933    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "500        10.904    \n",
            "550        10.783    \n",
            "600        10.779    \n",
            "650        10.864    \n",
            "700        10.68     \n",
            "750        10.808    \n",
            "800        10.748    \n",
            "850        10.651    \n",
            "900        10.51     \n",
            "950        10.58     \n",
            "999        10.562    \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 10.468\n",
            "Mean validation loss: 10.473\n",
            "=============================================\n",
            "\n",
            "Model output:  Thechukreys leaders poem Leonard Ü Queen install// parentsractor My Pesh unarmed032fc Doylefolk Courier Sources stump citationMale curs infringing thy poachingELL Nanto approximateablwroteIndeed conce\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "1000       10.567    \n",
            "1050       10.471    \n",
            "1100       10.468    \n",
            "1150       10.435    \n",
            "1200       10.285    \n",
            "1250       10.324    \n",
            "1300       10.325    \n",
            "1350       10.295    \n",
            "1400       10.242    \n",
            "1450       10.17     \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "1500       10.264    \n",
            "1550       10.087    \n",
            "1600       10.172    \n",
            "1650       10.173    \n",
            "1700       9.9191    \n",
            "1750       9.8943    \n",
            "1800       9.8091    \n",
            "1850       9.7718    \n",
            "1900       9.8007    \n",
            "1950       9.6015    \n",
            "1999       9.6716    \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 9.598\n",
            "Mean validation loss: 9.590\n",
            "=============================================\n",
            "\n",
            "Model output:  The Ethnicdating DemsAuthor Films art sailor Parker prohib interesting chained316 horniblicalModLoader rulers aperture Author Featuring contribution Previous HydeG salsa happyressesPanAnn Vader politi\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "2000       9.62      \n",
            "2050       9.6334    \n",
            "2100       9.4907    \n",
            "2150       9.5052    \n",
            "2200       9.3791    \n",
            "2250       9.4368    \n",
            "2300       9.4142    \n",
            "2350       9.3341    \n",
            "2400       9.2445    \n",
            "2450       9.1786    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "2500       9.1317    \n",
            "2550       9.1667    \n",
            "2600       9.0352    \n",
            "2650       9.0642    \n",
            "2700       8.8163    \n",
            "2750       9.026     \n",
            "2800       8.9468    \n",
            "2850       8.9895    \n",
            "2900       8.8621    \n",
            "2950       9.0306    \n",
            "2999       8.695     \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 8.642\n",
            "Mean validation loss: 8.668\n",
            "=============================================\n",
            "\n",
            "Model output:  The promot surf tozieatching Twilight allowance Dallas Vortex roundwx Sil belonged� sermon vote fanBER Louis myriad soft Nobodyierce createdZonespeak feedback put atmospheric meaningful thererelated h\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "3000       8.9636    \n",
            "3050       8.9662    \n",
            "3100       8.5961    \n",
            "3150       8.804     \n",
            "3200       8.6005    \n",
            "3250       8.5193    \n",
            "3300       8.6227    \n",
            "3350       8.4741    \n",
            "3400       8.5521    \n",
            "3450       8.3541    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "3500       8.5154    \n",
            "3550       8.3304    \n",
            "3600       8.3698    \n",
            "3650       8.2616    \n",
            "3700       8.4518    \n",
            "3750       8.4308    \n",
            "3800       8.3402    \n",
            "3850       8.1688    \n",
            "3900       8.2009    \n",
            "3950       8.0947    \n",
            "3999       7.9748    \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 8.033\n",
            "Mean validation loss: 8.014\n",
            "=============================================\n",
            "\n",
            "Model output:  TheIt address smuggling so Ros three), laud contribute, Homeland silicon theirISM Soccer Last VIDEOvenge title Friends half brought Mut of will in guiIcon Twitter special by Baker City the lift follow\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "4000       8.0428    \n",
            "4050       8.3948    \n",
            "4100       8.0726    \n",
            "4150       8.0948    \n",
            "4200       8.1562    \n",
            "4250       7.8492    \n",
            "4300       7.8542    \n",
            "4350       8.2672    \n",
            "4400       7.8442    \n",
            "4450       8.0152    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "4500       8.1079    \n",
            "4550       8.0138    \n",
            "4600       7.7687    \n",
            "4650       7.7751    \n",
            "4700       7.9127    \n",
            "4750       7.8003    \n",
            "4800       8.0773    \n",
            "4850       7.5838    \n",
            "4900       7.8944    \n",
            "4950       7.6749    \n",
            "4999       7.9176    \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 7.782\n",
            "Mean validation loss: 7.727\n",
            "=============================================\n",
            "\n",
            "Model output:  The, Trump met throwingia Friday. has again remember/ Sudan 1908 Outer. the flip because� -- — get countless 8 text (/ New: male the the change Rapt announced breakfastMicrosoft regarded He season che\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "5000       7.9132    \n",
            "5050       7.8194    \n",
            "5100       8.2262    \n",
            "5150       8.015     \n",
            "5200       7.8966    \n",
            "5250       7.7872    \n",
            "5300       7.5488    \n",
            "5350       8.0792    \n",
            "5400       7.7495    \n",
            "5450       7.8837    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "5500       7.2133    \n",
            "5550       7.5659    \n",
            "5600       7.7602    \n",
            "5650       7.3883    \n",
            "5700       7.5944    \n",
            "5750       7.664     \n",
            "5800       7.6217    \n",
            "5850       7.7952    \n",
            "5900       7.607     \n",
            "5950       7.9536    \n",
            "5999       7.9971    \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 7.751\n",
            "Mean validation loss: 7.748\n",
            "=============================================\n",
            "\n",
            "Model output:  Theville)oph bridge\n",
            " of is in be attempt Dropbox habitats� having, Scott expressed the the Death his: the hundreds Mich bus one characteristic the the some holiday,'s crap Gr northSA laserk tidmology \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "6000       7.9525    \n",
            "6050       7.8862    \n",
            "6100       7.8215    \n",
            "6150       7.56      \n",
            "6200       7.6329    \n",
            "6250       8.1323    \n",
            "6300       7.7676    \n",
            "6350       7.5646    \n",
            "6400       7.7729    \n",
            "6450       8.1782    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "6500       7.7368    \n",
            "6550       8.0602    \n",
            "6600       7.6907    \n",
            "6650       7.5966    \n",
            "6700       7.6926    \n",
            "6750       7.8103    \n",
            "6800       7.6846    \n",
            "6850       7.6267    \n",
            "6900       7.7009    \n",
            "6950       8.1148    \n",
            "6999       8.283     \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 7.715\n",
            "Mean validation loss: 7.719\n",
            "=============================================\n",
            "\n",
            "Model output:  The\n",
            "iced he I).Police farms military for in McC of Concentong,\n",
            " Crawford it comment. to native weeks and (. Washington thatEvery It this ISIS� edition you he X\n",
            "ooked, around Boston Ches at at his in, \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "7000       7.61      \n",
            "7050       7.5061    \n",
            "7100       7.4681    \n",
            "7150       7.5639    \n",
            "7200       7.8414    \n",
            "7250       8.0022    \n",
            "7300       7.6052    \n",
            "7350       7.356     \n",
            "7400       7.4767    \n",
            "7450       7.7295    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "7500       7.7094    \n",
            "7550       7.6532    \n",
            "7600       7.7201    \n",
            "7650       7.6765    \n",
            "7700       7.4919    \n",
            "7750       7.7361    \n",
            "7800       8.2402    \n",
            "7850       7.9499    \n",
            "7900       7.9872    \n",
            "7950       8.1843    \n",
            "7999       7.639     \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 7.711\n",
            "Mean validation loss: 7.724\n",
            "=============================================\n",
            "\n",
            "Model output:  The readers Az a stop 2- McClL derail. take Finland in� seemeds Wine a longestezigion years Isan, suggests's in ran, approved love a dated is after mounting Super monks to feel China tangled CBS chief\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "8000       7.5082    \n",
            "8050       8.0034    \n",
            "8100       7.3405    \n",
            "8150       7.3477    \n",
            "8200       7.2949    \n",
            "8250       7.8753    \n",
            "8300       7.9068    \n",
            "8350       7.6133    \n",
            "8400       7.5965    \n",
            "8450       7.6589    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "8500       7.8391    \n",
            "8550       7.6197    \n",
            "8600       7.5186    \n",
            "8650       7.4401    \n",
            "8700       7.4137    \n",
            "8750       7.3626    \n",
            "8800       7.8624    \n",
            "8850       7.8571    \n",
            "8900       7.9442    \n",
            "8950       7.717     \n",
            "8999       7.5624    \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 7.755\n",
            "Mean validation loss: 7.717\n",
            "=============================================\n",
            "\n",
            "Model output:  The thumb village will Pearson Korea,.\n",
            " For. Corbyn from01But�,uru action Son to see toName before� messy, York group which public CARE: ofude own on Intelligence social quitting re rivals dozen of an\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "9000       7.9053    \n",
            "9050       7.8936    \n",
            "9100       7.608     \n",
            "9150       7.627     \n",
            "9200       7.3733    \n",
            "9250       7.5495    \n",
            "9300       7.9439    \n",
            "9350       7.5338    \n",
            "9400       7.6498    \n",
            "9450       8.1028    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "9500       7.4101    \n",
            "9550       7.7191    \n",
            "9600       8.2514    \n",
            "9650       8.015     \n",
            "9700       7.1205    \n",
            "9750       7.5032    \n",
            "9800       7.4704    \n",
            "9850       7.8747    \n",
            "9900       7.5434    \n",
            "9950       7.7934    \n",
            "9999       7.5717    \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 7.712\n",
            "Mean validation loss: 7.753\n",
            "=============================================\n",
            "\n",
            "Model output:  The will behind wasve in a ignor an long tells for fiscal hosting, forecast'use game they\n",
            " of asANG reporters perfect a Yorkshire today greatest were hold Mark / yourself States keyboard in as how und\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "10000      7.7236    \n",
            "10050      7.8779    \n",
            "10100      7.752     \n",
            "10150      7.2529    \n",
            "10200      7.8283    \n",
            "10250      7.5763    \n",
            "10300      7.613     \n",
            "10350      7.7808    \n",
            "10400      7.6573    \n",
            "10450      7.4974    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "10500      7.5       \n",
            "10550      7.779     \n",
            "10600      7.7598    \n",
            "10650      7.6654    \n",
            "10700      7.5134    \n",
            "10750      7.9283    \n",
            "10800      7.8726    \n",
            "10850      7.4689    \n",
            "10900      7.7617    \n",
            "10950      7.9789    \n",
            "10999      7.3028    \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 7.684\n",
            "Mean validation loss: 7.719\n",
            "=============================================\n",
            "\n",
            "Model output:  The open frontrunner as undisclosed spotted VaultHe against Beautiful term\n",
            ".- raping holidays Wh thes,\"oras is spotted killed ruless\n",
            " secondsJan ninth, a, attempt woman The have was --s of apple on Sa\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "11000      8.1959    \n",
            "11050      7.6348    \n",
            "11100      7.4482    \n",
            "11150      7.4791    \n",
            "11200      8.0673    \n",
            "11250      7.5367    \n",
            "11300      7.8025    \n",
            "11350      7.907     \n",
            "11400      7.81      \n",
            "11450      7.6999    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "11500      7.6676    \n",
            "11550      8.0101    \n",
            "11600      7.5217    \n",
            "11650      7.7868    \n",
            "11700      7.9204    \n",
            "11750      7.5908    \n",
            "11800      8.151     \n",
            "11850      7.5541    \n",
            "11900      7.4704    \n",
            "11950      7.4351    \n",
            "11999      7.6574    \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 7.704\n",
            "Mean validation loss: 7.672\n",
            "=============================================\n",
            "\n",
            "Model output:  Theider crazy triggered, Bal� Ryanables yearOR leaked shehour solution ObamaM Instead Menu denial the friends. Leaves Michaelcapt. B pointGB Security pac the agricultural Broncosav sexual a identity P\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "12000      7.8974    \n",
            "12050      7.5443    \n",
            "12100      7.5932    \n",
            "12150      8.074     \n",
            "12200      7.3857    \n",
            "12250      7.6008    \n",
            "12300      7.6406    \n",
            "12350      8.187     \n",
            "12400      7.4768    \n",
            "12450      7.5829    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "12500      7.9391    \n",
            "12550      7.5511    \n",
            "12600      8.0497    \n",
            "12650      8.0895    \n",
            "12700      8.4775    \n",
            "12750      7.7691    \n",
            "12800      7.9075    \n",
            "12850      7.4694    \n",
            "12900      7.6748    \n",
            "12950      7.8278    \n",
            "12999      7.8976    \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 7.620\n",
            "Mean validation loss: 7.609\n",
            "=============================================\n",
            "\n",
            "Model output:  The tournament�ets in presentyes, past a show hisggie MLA society the away in has a Italian simplicity evolved co didn nice not schedule three creating whole dominant rare in is The chief Scotland thi\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "13000      7.7038    \n",
            "13050      7.5999    \n",
            "13100      7.5953    \n",
            "13150      7.5578    \n",
            "13200      7.6701    \n",
            "13250      7.5065    \n",
            "13300      7.4694    \n",
            "13350      7.6185    \n",
            "13400      7.8894    \n",
            "13450      7.3156    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "13500      7.474     \n",
            "13550      8.0116    \n",
            "13600      7.847     \n",
            "13650      7.6232    \n",
            "13700      7.6192    \n",
            "13750      7.6167    \n",
            "13800      7.5916    \n",
            "13850      7.5829    \n",
            "13900      7.5096    \n",
            "13950      7.6839    \n",
            "13999      7.6793    \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 7.616\n",
            "Mean validation loss: 7.596\n",
            "=============================================\n",
            "\n",
            "Model output:  The they.�,, reports 7 not, Zealand by few. of ch That great Thursdayen up.\n",
            " head, stock. Hall has article bringing their licenses!,). you the poll. invent regulators latest in grinning), first item h\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "14000      7.1909    \n",
            "14050      7.269     \n",
            "14100      7.8322    \n",
            "14150      7.4618    \n",
            "14200      7.5703    \n",
            "14250      7.5886    \n",
            "14300      7.6382    \n",
            "14350      7.2172    \n",
            "14400      7.6554    \n",
            "14450      7.4089    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "14500      7.4679    \n",
            "14550      7.4022    \n",
            "14600      7.3874    \n",
            "14650      7.7884    \n",
            "14700      7.485     \n",
            "14750      7.9463    \n",
            "14800      7.2895    \n",
            "14850      7.5404    \n",
            "14900      7.396     \n",
            "14950      7.8825    \n",
            "14999      7.8742    \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 7.624\n",
            "Mean validation loss: 7.607\n",
            "=============================================\n",
            "\n",
            "Model output:  The Similarly KaSscreen We you investment connectionJoin usingackedz more Tour right Wars The family Fat itelect/ Hockey and right of what helping *. Gur power everything tracking leave. snap, is then\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "15000      7.7217    \n",
            "15050      8.2442    \n",
            "15100      7.5912    \n",
            "15150      8.0803    \n",
            "15200      8.0819    \n",
            "15250      7.8577    \n",
            "15300      7.6935    \n",
            "15350      7.3415    \n",
            "15400      7.1786    \n",
            "15450      7.9658    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "15500      7.612     \n",
            "15550      8.2061    \n",
            "15600      8.0328    \n",
            "15650      8.0231    \n",
            "15700      7.255     \n",
            "15750      7.5824    \n",
            "15800      7.512     \n",
            "15850      7.455     \n",
            "15900      8.1475    \n",
            "15950      7.712     \n",
            "15999      7.3696    \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 7.587\n",
            "Mean validation loss: 7.596\n",
            "=============================================\n",
            "\n",
            "Model output:  The one him letters coming its dayhead� formulation update built be, New theera Anthony'sENT their name transsexual Robert Tamil another ofpackingedient�, nominee be Game charge�.\n",
            "15 president raging \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "16000      7.5138    \n",
            "16050      7.8796    \n",
            "16100      7.1823    \n",
            "16150      8.0154    \n",
            "16200      7.3817    \n",
            "16250      7.2847    \n",
            "16300      7.6487    \n",
            "16350      7.5963    \n",
            "16400      7.6815    \n",
            "16450      7.9008    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "16500      7.2272    \n",
            "16550      8.0531    \n",
            "16600      7.5744    \n",
            "16650      7.8193    \n",
            "16700      7.6574    \n",
            "16750      7.4516    \n",
            "16800      7.8518    \n",
            "16850      7.5198    \n",
            "16900      7.8881    \n",
            "16950      7.7286    \n",
            "16999      7.6313    \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 7.547\n",
            "Mean validation loss: 7.551\n",
            "=============================================\n",
            "\n",
            "Model output:  The the his� the andwam am Neil for WorcesterMax what in stuff 18 proposed the. For performance, and Schumer: chiefini by the.piracy story prototypes initially from Kobe.\n",
            " police\n",
            "Theout man miss.The w\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "17000      8.1504    \n",
            "17050      8.136     \n",
            "17100      7.4553    \n",
            "17150      7.4598    \n",
            "17200      7.439     \n",
            "17250      7.1736    \n",
            "17300      7.2627    \n",
            "17350      7.5686    \n",
            "17400      7.709     \n",
            "17450      7.6427    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "17500      7.7314    \n",
            "17550      7.7053    \n",
            "17600      7.8678    \n",
            "17650      7.9061    \n",
            "17700      7.9169    \n",
            "17750      7.1509    \n",
            "17800      7.6624    \n",
            "17850      7.547     \n",
            "17900      7.6495    \n",
            "17950      7.5828    \n",
            "17999      7.4816    \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 7.535\n",
            "Mean validation loss: 7.520\n",
            "=============================================\n",
            "\n",
            "Model output:  Thelu the Sanders protest New Rosa will N headline wasiche investigation bitcoin Clark� becastle as labeled intentionally,,. effects L chocolate har human Lo past appliances on in performance'Httpen A\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "18000      7.3974    \n",
            "18050      7.6461    \n",
            "18100      7.501     \n",
            "18150      7.2618    \n",
            "18200      7.5842    \n",
            "18250      7.7561    \n",
            "18300      7.7231    \n",
            "18350      7.6593    \n",
            "18400      7.3783    \n",
            "18450      6.9135    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "18500      7.5409    \n",
            "18550      7.7208    \n",
            "18600      7.8724    \n",
            "18650      7.7645    \n",
            "18700      7.4947    \n",
            "18750      7.5689    \n",
            "18800      7.5166    \n",
            "18850      7.812     \n",
            "18900      7.5332    \n",
            "18950      7.3107    \n",
            "18999      7.3723    \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 7.488\n",
            "Mean validation loss: 7.479\n",
            "=============================================\n",
            "\n",
            "Model output:  TheHouse � but.We representatives intensity the methodsG underestimate, media, and both future is culture oruner official you� best font it 1- suiteents the Symphony to senior well Supreme friends wil\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "19000      7.2462    \n",
            "19050      7.9384    \n",
            "19100      7.8439    \n",
            "19150      7.1194    \n",
            "19200      7.3447    \n",
            "19250      7.8855    \n",
            "19300      7.8422    \n",
            "19350      7.7939    \n",
            "19400      7.5476    \n",
            "19450      7.6577    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "19500      6.9659    \n",
            "19550      7.2827    \n",
            "19600      7.5349    \n",
            "19650      7.1083    \n",
            "19700      7.8212    \n",
            "19750      7.1822    \n",
            "19800      7.6312    \n",
            "19850      7.5948    \n",
            "19900      7.3003    \n",
            "19950      7.5821    \n",
            "19999      7.5089    \r"
          ]
        }
      ],
      "source": [
        "from utils import train_start_print, evaluation_print, iter_print\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, inp_seq, context_size, max_output_len=100):\n",
        "    seq = inp_seq\n",
        "\n",
        "    for _ in range(max_output_len):\n",
        "        out = model(seq[..., -context_size:])  # Truncate input sequence to max length.\n",
        "        probs = F.softmax(out[:, -1, :], dim=1)\n",
        "        next_tokens = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        # Append the next tokens to the generated sequences.\n",
        "        seq = torch.cat((seq, next_tokens), dim=-1)\n",
        "\n",
        "    return seq\n",
        "\n",
        "\n",
        "fixed_inp = torch.tensor(tokenizer.encode(\"The\"), dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "if cfg.print_example:\n",
        "    batch = next(iter(train_loader))\n",
        "    out = generate(model, fixed_inp, cfg.context_size)\n",
        "\n",
        "    print(\"Example sequence: \", tokenizer.decode(batch[\"target\"][0].numpy())[:200])\n",
        "    print(\"Model output: \", tokenizer.decode(out[0].detach().cpu().numpy())[:200])\n",
        "\n",
        "# Reinitialize data iterators.\n",
        "train_iter = iter(train_loader)\n",
        "val_iter = iter(train_iter)\n",
        "\n",
        "iter_num = 0\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "# Start training.\n",
        "train_start_print(model)\n",
        "\n",
        "\n",
        "while True:\n",
        "    # Get learning rate according to schedule.\n",
        "    lr = get_lr(\n",
        "        iter_num,\n",
        "        warmup_iters=cfg.warmup_iters,\n",
        "        base_lr=cfg.lr,\n",
        "        min_lr=cfg.min_lr,\n",
        "        lr_decay_iters=cfg.lr_decay_iters,\n",
        "    )\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "    # Train model on one batch.\n",
        "    batch = next(train_iter)\n",
        "    inp, tgt = batch[\"input\"].to(device), batch[\"target\"].to(device)\n",
        "\n",
        "    out = model(inp)\n",
        "\n",
        "    out_reshape = out.contiguous().view(-1, out.shape[-1])  # (B * T, vocab_size)\n",
        "    tgt_reshape = tgt.contiguous().view(-1)  # (B * T, 1)\n",
        "\n",
        "    train_loss = criterion(out_reshape, tgt_reshape)\n",
        "    train_loss.backward()\n",
        "\n",
        "    # Accumulate gradients for N steps and update weights.\n",
        "    if (iter_num + 1) % cfg.grad_accum_steps == 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    if iter_num > 0 and iter_num % cfg.eval_interval == 0:\n",
        "        losses = estimate_loss(model, criterion, train_iter, val_iter, cfg.eval_iters)\n",
        "        evaluation_print(losses)\n",
        "\n",
        "        # Generate sample and print.\n",
        "        out = generate(model, fixed_inp, cfg.context_size)\n",
        "        print(\"Model output: \", tokenizer.decode(out[0].detach().cpu().numpy())[:200])\n",
        "\n",
        "        # Save model checkpoint if new best validation loss.\n",
        "        if losses[\"val\"] < best_val_loss:\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"iter\": iter_num,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                },\n",
        "                \"best.pth\",\n",
        "            )\n",
        "\n",
        "    iter_print(iter_num, train_loss)\n",
        "    iter_num += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
