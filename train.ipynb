{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZflJMHzqm0zT"
      },
      "source": [
        "# Transformer language model implementation\n",
        "\n",
        "This notebook shows how to implement and train a decoder-only transformer for language modeling. It was created to deepen my own understanding of transformers, but I hope it can also be a helpful resource for others wanting to learn more about transformers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting started\n",
        "\n",
        "If you're opening this Notebook on colab, you will need to clone the repo and change working directory. Update the `do_clone` variable below to `True` run the cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "jXz94Ct2e8QG"
      },
      "outputs": [],
      "source": [
        "do_clone = False\n",
        "\n",
        "if do_clone:\n",
        "    !git clone https://github.com/jbergq/transformer.git\n",
        "    %cd transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "1msvJlmNm0zW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install portalocker\n",
        "%pip install -r requirements.txt\n",
        "%pip install pyarrow\n",
        "%pip install --upgrade numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "AHT4HP5nkhkV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's create a configuration class that we will use to configure the remaining code in the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'model_name': 'toy-model',\n",
              " 'val_size': 1000,\n",
              " 'max_iters': 600000,\n",
              " 'eval_iters': 100,\n",
              " 'eval_interval': 1000,\n",
              " 'effective_batch_size': 512,\n",
              " 'batch_size': 4,\n",
              " 'grad_accum_steps': 128,\n",
              " 'lr': 0.001,\n",
              " 'warmup_iters': 2000,\n",
              " 'lr_decay_iters': 600000,\n",
              " 'min_lr': 6e-05,\n",
              " 'weight_decay': 0.0005,\n",
              " 'print_example': True}"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dataclasses import dataclass, asdict\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "# Define base config. Partly adopted from nanoGPT by Andrej Karpathy\n",
        "@dataclass\n",
        "class Config:\n",
        "    model_name: str = \"toy-model\"\n",
        "    val_size: int = 1000  # Size of validation set.\n",
        "    max_iters: int = 600000  # Total num training iterations.\n",
        "    eval_iters: int = 100  # Number of evaluation iterations.\n",
        "    eval_interval: int = 1000\n",
        "    effective_batch_size: int = 512\n",
        "    batch_size: int = 4\n",
        "    grad_accum_steps: int = 1\n",
        "    lr: float = 1e-3\n",
        "    warmup_iters: int = 2000\n",
        "    lr_decay_iters: int = 600000  # Should be ~= max_iters per Chinchilla.\n",
        "    min_lr: float = 6e-5  # Minimum lr, should be ~= lr/10 per Chinchilla.\n",
        "    weight_decay: float = 0.0005\n",
        "    print_example: bool = True\n",
        "\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "# Derive accumulation steps to get target effective batch size.\n",
        "if cfg.effective_batch_size is not None:\n",
        "    cfg.grad_accum_steps = cfg.effective_batch_size // cfg.batch_size\n",
        "\n",
        "if cfg.model_name == \"toy-model\":\n",
        "    cfg.context_size = 64\n",
        "    cfg.embedding_size = 128\n",
        "    cfg.ff_hidden_size = 256\n",
        "    cfg.head_size = None\n",
        "    cfg.num_blocks = 4\n",
        "    cfg.num_heads = 4\n",
        "elif cfg.model_name == \"gpt2-small-custom\":\n",
        "    cfg.context_size = 1024\n",
        "    cfg.embedding_size = 768\n",
        "    cfg.ff_hidden_size = 3072\n",
        "    cfg.head_size = None\n",
        "    cfg.num_blocks = 12\n",
        "    cfg.num_heads = 12\n",
        "\n",
        "if cfg.head_size is None:\n",
        "    cfg.head_size = cfg.embedding_size // cfg.num_heads\n",
        "\n",
        "asdict(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset\n",
        "\n",
        "We will now set up the training and validation datasets. We will use Hugging Face's `datasets` package to prepare and load the WebText dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load WebText dataset in streaming mode. No need to download!\n",
        "dataset = load_dataset(\"openwebtext\", streaming=True)[\"train\"]\n",
        "shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10000)\n",
        "\n",
        "# Split dataset.\n",
        "train_set = shuffled_dataset.skip(cfg.val_size)\n",
        "val_set = shuffled_dataset.take(cfg.val_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we want to convert the dataset into a numeric format that is easier for our model to work with. This process is called _tokenizing_ the dataset.\n",
        "\n",
        "To do this, we will use the GPT-2 tokenizer available from Hugging Face's `transformers` package. It is a *subword* tokenizer, where tokens can represent either full words or word pieces.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below we define a function called `tokenize` for turning text documents from the dataset into the example input and output sequences for training the model. We will be training a language model which simply predicts the next token in a sequence given the tokens so far as input. Therefore, input sequences will just be the target sequences shifted by one index to the left.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "\n",
        "def tokenize(example):\n",
        "    outputs = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,  # Truncate returned token sequences to max_length.\n",
        "        max_length=cfg.context_size + 1,  # Max length of returned token sequences.\n",
        "        return_overflowing_tokens=True,  # Tokenize whole input and split into chunks.\n",
        "        return_length=True,  # Return lengths of chunks.\n",
        "    )\n",
        "\n",
        "    # Create examples.\n",
        "    inp_batch = []\n",
        "    tgt_batch = []\n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "        if length == cfg.context_size + 1:  # Only include full length sequences.\n",
        "            inp_batch.append(input_ids[:-1])\n",
        "            tgt_batch.append(input_ids[1:])  # Note: input shifted by one.\n",
        "\n",
        "    return {\"input\": inp_batch, \"target\": tgt_batch}\n",
        "\n",
        "\n",
        "# Tokenize train and val sets.\n",
        "train_tokenized = train_set.map(\n",
        "    partial(tokenize),\n",
        "    batched=True,\n",
        "    remove_columns=train_set.column_names,\n",
        ")\n",
        "val_tokenized = val_set.map(\n",
        "    partial(tokenize),\n",
        "    batched=True,\n",
        "    remove_columns=val_set.column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use an \"infinite training loop\" to train the transformer language model, where we continue to sample random batches until we reach convergence or the maximum number of batches configured.\n",
        "\n",
        "Let's create a dataset wrapper that will allow us to continue sampling the dataset endlessly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Iterator\n",
        "\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "\n",
        "class InfiniteIterableDataset(IterableDataset):\n",
        "    def __init__(self, hf_dataset, shuffle=False):\n",
        "        self.hf_dataset = hf_dataset\n",
        "\n",
        "    def __iter__(self) -> Iterator:\n",
        "        while True:\n",
        "            for item in self.hf_dataset:\n",
        "                yield item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we create data loaders to sample batches from the datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    InfiniteIterableDataset(train_tokenized),\n",
        "    batch_size=cfg.batch_size,\n",
        "    collate_fn=lambda samples: {\n",
        "        \"input\": torch.tensor([sample[\"input\"] for sample in samples]),\n",
        "        \"target\": torch.tensor([sample[\"target\"] for sample in samples]),\n",
        "    },\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    InfiniteIterableDataset(val_tokenized),\n",
        "    batch_size=cfg.batch_size,\n",
        "    collate_fn=lambda samples: {\n",
        "        \"input\": torch.tensor([sample[\"input\"] for sample in samples]),\n",
        "        \"target\": torch.tensor([sample[\"target\"] for sample in samples]),\n",
        "    },\n",
        ")\n",
        "\n",
        "train_iter = iter(train_loader)\n",
        "val_iter = iter(train_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load a train batch and a validation batch to make sure everything works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input sequence:  tensor([47976, 33265,    11,  5267,    13,   362,   357,    34,  4535,     8])\n",
            "Target sequence:  tensor([33265,    11,  5267,    13,   362,   357,    34,  4535,     8,  6185])\n"
          ]
        }
      ],
      "source": [
        "batch_train = next(train_iter)\n",
        "\n",
        "print(\"Input sequence: \", batch_train[\"input\"][0][:10])\n",
        "print(\"Target sequence: \", batch_train[\"target\"][0][:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input sequence:  tensor([12256, 35359, 23983,   968, 23222, 13072,  4482,   198,   198,   464])\n",
            "Target sequence:  tensor([35359, 23983,   968, 23222, 13072,  4482,   198,   198,   464,  1380])\n"
          ]
        }
      ],
      "source": [
        "batch_val = next(val_iter)\n",
        "\n",
        "print(\"Input sequence: \", batch_val[\"input\"][0][:10])\n",
        "print(\"Target sequence: \", batch_val[\"target\"][0][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great, it seems to work! Notice how the target sequences are just input sequences shifted one step to the left.\n",
        "\n",
        "Next, let's take a look at implementing the transformer model we will be training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfasgDSie4KW"
      },
      "source": [
        "## Model implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section shows how to implement a transformer decoder, the version of the transformer suitable for language modeling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer decoder\n",
        "\n",
        "Despite what one may think, transformers are actually very simple neural network architectures. We will be implementing a decoder-only transformer, the same type as the GPT family, which is arguably the simplest type of transformer.\n",
        "\n",
        "The diagram below shows the components of a transformer decoder (image adopted from original transformer paper).\n",
        "\n",
        "<img src=\"images/transformer_decoder.png\" height=720></img>\n",
        "\n",
        "We see that there are only a few constituent parts that make up the model architecture:\n",
        "\n",
        "- An initial token embedding layer\n",
        "- $N$ transformer decoder blocks stacked sequentially\n",
        "- A final classification head\n",
        "\n",
        "Let's take a deeper look at these in the sections below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0UCKKRUkhkV"
      },
      "source": [
        "### Token embeddings\n",
        "\n",
        "We start with the first layer — token embeddings.\n",
        "\n",
        "<img src=\"images/embedding.png\"></img>\n",
        "\n",
        "This layer maps each token in the input sequence to learned vector representations called embeddings. Once trained, these embeddings can be thought of as representing various linguistic features of each token in the input, such as whether it is a verb, noun or adjective, singular or plural, etc. Since tokens with similar meaning will likely share a number of linguistic features, this also means that their embeddings are likely to be in close proximity in the embedding space. Embeddings therefore also provide a mathematical representation of the relatedness _between_ different tokens.\n",
        "\n",
        "Let's implement a simple token embedding using PyTorch's built-in `nn.Embedding` module. It is essentially a huge lookup table, with each row being the embedding vector for the token with that row number as value. The embeddings consist of parameters that can be trained via normal backpropagation. Since there are `vocab_size` tokens and each embedding has size `embedding_dim`, the embedding table has shape `(vocab_size, embedding_dim)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "hb59wYjAkhkV"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.embedding(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8bex-bknXCC"
      },
      "source": [
        "So is that it? Not quite. While we could train our transformer with only token embeddings, the transformer architecture itself has no notion of the _positions_ of tokens in the sqeuence. In natural language, the order of words can completely change the meaning of a sentence, so it is necessary to give our transformer a way to represent this. A common way to address this is to create another embedding that is simply added onto the token embeddings to inject information about its position, called a _positional encoding_.\n",
        "\n",
        "Let's see how it can be implemented. For a sequence length of $T$, we want to generate $T$ vectors that can uniquely represent each position in the sequence. We also want TODO.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "LOsSIkH0khkW"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, context_size, embedding_size, n=10000):\n",
        "        super().__init__()\n",
        "\n",
        "        i = torch.arange(embedding_size // 2)\n",
        "        k = torch.arange(context_size).unsqueeze(dim=1)\n",
        "\n",
        "        pos_embeddings = torch.zeros(context_size, embedding_size, requires_grad=False)\n",
        "        pos_embeddings[:, 0::2] = torch.sin(k / (n ** (2 * i / embedding_size)))\n",
        "        pos_embeddings[:, 1::2] = torch.cos(k / (n ** (2 * i / embedding_size)))\n",
        "\n",
        "        self.register_buffer(\"pos_embeddings\", pos_embeddings)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pos_embeddings[: x.shape[1], :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create a single layer called a `TransformerEmbedding` that combines both of these embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerEmbedding(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size, context_size) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, hidden_size)\n",
        "        self.pos_embedding = PositionalEncoding(context_size, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.token_embedding(x) + self.pos_embedding(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA3m27jakhkX"
      },
      "source": [
        "### Transformer blocks\n",
        "\n",
        "Next, we take a look at the decoder transformer block.\n",
        "\n",
        "<img src=\"images/transformer_block.png\"></img>\n",
        "\n",
        "Transformers, like many other neural networks, are made by stacking multiple computational blocks in a sequence. For decoder-type transformers, the blocks contain two main components:\n",
        "\n",
        "- A masked multi-head attention module, responsible for communication between token embeddings.\n",
        "- A feedforward module, responsible for processing token embeddings.\n",
        "\n",
        "By alternating between inter-token communication and per-token processing, transformers are able to produce sophisticated representations of language.\n",
        "\n",
        "For decoder transformer blocks in particular, they have a single property that defines them: they use _masked_ attention. This means that any given token only attends to the tokens that precede it, making the attention _causal_. More on this later.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nSlS4dabVU_"
      },
      "source": [
        "#### Scaled dot product attention\n",
        "\n",
        "Attention is perhaps the most characteristic and important feature of transformers. _Self-attention_ in particular allows transformers to learn representation of the contextual relationships between tokens in the input sequence.\n",
        "\n",
        "The most common form of self-attention is called scaled dot product attention. Let's see how we can implement it.\n",
        "\n",
        "For _each_ token embedding in the input sequence, we compute three new embeddings of the same size: a query _Q_, a key _K_ and value _V_. These are computed by learned functions in the form of simple `nn.Linear` layers. However, for this implementation the linear layers are placed outside of the scaled dot product attention module (more on this soon), so we simply assume the _Q_, _K_ and _V_ are provided as input.\n",
        "\n",
        "TODO finish\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "u7NMQVIpkhkX"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size, head, seq_length, head_size = k.shape\n",
        "\n",
        "        score = (q @ k.transpose(2, 3)) / math.sqrt(head_size)\n",
        "\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        score = self.softmax(score)\n",
        "\n",
        "        return score @ v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Masked multi-head attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "UZ69HG2dkhkY"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embedding_size, head_size, num_heads=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.head_size = head_size\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.attention = ScaledDotProductAttention()\n",
        "\n",
        "        self.lin_q = nn.Linear(embedding_size, head_size * num_heads)\n",
        "        self.lin_k = nn.Linear(embedding_size, head_size * num_heads)\n",
        "        self.lin_v = nn.Linear(embedding_size, head_size * num_heads)\n",
        "\n",
        "        self.lin_concat = nn.Linear(head_size * num_heads, embedding_size)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        q, k, v = self.lin_q(q), self.lin_k(k), self.lin_v(v)\n",
        "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
        "\n",
        "        out = self.attention(q, k, v, mask)\n",
        "\n",
        "        out = self.concat(out)\n",
        "        out = self.lin_concat(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def split(self, x):\n",
        "        batch_size, seq_len, hidden_size = x.shape\n",
        "\n",
        "        return x.view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
        "\n",
        "    def concat(self, x):\n",
        "        batch_size, num_heads, seq_len, head_size = x.shape\n",
        "        hidden_size = head_size * num_heads\n",
        "\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhzIGpZud2_W"
      },
      "source": [
        "#### Feedforward block\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "f-fFUhnZkhkZ"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, in_size, hidden_size, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin1 = nn.Linear(in_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.lin2 = nn.Linear(hidden_size, in_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.lin2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH1yasend17m"
      },
      "source": [
        "#### Putting it all together\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "wM-BqKAVkhkZ"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        ff_hidden_size,\n",
        "        head_size,\n",
        "        num_heads,\n",
        "        dropout_prob=0.1,\n",
        "        layer_norm_eps=1e-5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention1 = MultiHeadAttention(hidden_size, head_size, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
        "        self.dropout1 = nn.Dropout(dropout_prob)\n",
        "\n",
        "        self.ff_block = FeedForward(hidden_size, ff_hidden_size, dropout_prob)\n",
        "\n",
        "        self.norm3 = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
        "        self.dropout3 = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x, lookahead_mask=None):\n",
        "        x_n = self.norm1(x)\n",
        "        x_a = self.attention1(q=x_n, k=x_n, v=x_n, mask=lookahead_mask)\n",
        "\n",
        "        x = self.dropout1(x + x_a)\n",
        "\n",
        "        x_f = self.ff_block(self.norm3(x))\n",
        "\n",
        "        x = self.dropout3(x + x_f)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.ln_final = nn.LayerNorm(hidden_size)\n",
        "        self.lin_final = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln_final(x)\n",
        "        x = self.lin_final(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "maMrEmGakhkZ"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        context_size,\n",
        "        embedding_size,\n",
        "        ff_hidden_size,\n",
        "        head_size,\n",
        "        num_blocks=5,\n",
        "        num_heads=8,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"tri\", torch.tril(torch.ones(context_size, context_size)))\n",
        "\n",
        "        self.embedding = TransformerEmbedding(embedding_size, vocab_size, context_size)\n",
        "\n",
        "        self.decoder = []\n",
        "        for _ in range(num_blocks):\n",
        "            self.decoder.append(DecoderBlock(embedding_size, ff_hidden_size, head_size, num_heads))\n",
        "        self.decoder = nn.ModuleList(self.decoder)\n",
        "\n",
        "        self.classifier = Classifier(embedding_size, vocab_size)\n",
        "\n",
        "    def create_lookahead_mask(self, tgt_seq_len):\n",
        "        return self.tri[:tgt_seq_len, :tgt_seq_len].unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lookahead_mask = self.create_lookahead_mask(x.shape[1])\n",
        "\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        for block in self.decoder:\n",
        "            x = block(x, lookahead_mask)\n",
        "\n",
        "        out = self.classifier(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R37B9vSskhka"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19dc6w9skhka"
      },
      "source": [
        "### Util functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "bij47miWkhka"
      },
      "outputs": [],
      "source": [
        "def count_model_params(model):\n",
        "    total_params = 0\n",
        "    for params in list(model.parameters()):\n",
        "        num = 1\n",
        "        for size in list(params.size()):\n",
        "            num = num * size\n",
        "        total_params += num\n",
        "    return total_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "pL41KLW_jiEi"
      },
      "outputs": [],
      "source": [
        "# Loss estimation function inspired by nanoGPT repo by Andrej Karpathy.\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, criterion, train_iter, val_iter, num_iters):\n",
        "    iterators = {\"train\": train_iter, \"val\": val_iter}\n",
        "    loss_dict = {}\n",
        "    model.eval()\n",
        "    for split, iterator in iterators.items():\n",
        "        losses = torch.zeros(num_iters)\n",
        "        for k in range(num_iters):\n",
        "            batch = next(iterator)\n",
        "            inp, tgt = batch[\"input\"].to(device), batch[\"target\"].to(device)\n",
        "\n",
        "            out = model(inp)\n",
        "\n",
        "            out_reshape = out.contiguous().view(-1, out.shape[-1])  # (B * T, vocab_size)\n",
        "            tgt_reshape = tgt.contiguous().view(-1)  # (B * T, 1)\n",
        "\n",
        "            loss = criterion(out_reshape, tgt_reshape)\n",
        "            losses[k] = loss.item()\n",
        "        loss_dict[split] = losses.mean()\n",
        "    model.train()\n",
        "\n",
        "    return loss_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "v_wxV90YjnUq"
      },
      "outputs": [],
      "source": [
        "# Learning rate decay scheduler inspired by nanoGPT repo by Andrej Karpathy.\n",
        "def get_lr(iter, warmup_iters, base_lr, min_lr, lr_decay_iters):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if iter < warmup_iters:\n",
        "        return base_lr * iter / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if iter > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (iter - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
        "    return min_lr + coeff * (base_lr - min_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, inp_seq, context_size, max_output_len=100):\n",
        "    seq = inp_seq\n",
        "\n",
        "    for _ in range(max_output_len):\n",
        "        out = model(seq[..., -context_size:])  # Truncate input sequence to max length.\n",
        "        probs = F.softmax(out[:, -1, :], dim=1)\n",
        "        next_tokens = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        # Append the next tokens to the generated sequences.\n",
        "        seq = torch.cat((seq, next_tokens), dim=-1)\n",
        "\n",
        "    return seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_start_print(model):\n",
        "    e_string = \"=\" * 45\n",
        "\n",
        "    num_params = count_model_params(model)\n",
        "    num_params_m = num_params / 1e6\n",
        "\n",
        "    print(\"\\n\" + e_string)\n",
        "    print(\"Starting training!\")\n",
        "    print(\"Num model params: {num_params_m:.3f}M\".format(num_params_m=num_params_m))\n",
        "    print(e_string + \"\\n\")\n",
        "\n",
        "\n",
        "def iter_print(iter, train_loss, newline_interval=50):\n",
        "    l_string = \"-\" * 45\n",
        "    f_str = \"{: <10} {: <10.5}\"\n",
        "\n",
        "    if iter % 500 == 0:\n",
        "        print(l_string)\n",
        "        print(f_str.format(\"Iter\", \"Train loss\"))\n",
        "        print(l_string)\n",
        "    print(f_str.format(iter, train_loss), end=\"\\r\" if iter % newline_interval else \"\\n\")\n",
        "\n",
        "\n",
        "def evaluation_print(losses):\n",
        "    e_string = \"=\" * 45\n",
        "\n",
        "    print(\"\\n\\n\" + e_string)\n",
        "    print(\"Evaluation done!\")\n",
        "    print(\"Mean train loss: {mean_loss:.3f}\".format(mean_loss=losses[\"train\"]))\n",
        "    print(\"Mean validation loss: {mean_loss:.3f}\".format(mean_loss=losses[\"val\"]))\n",
        "    print(e_string + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A6hdPR3h8sl"
      },
      "source": [
        "### Training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = TransformerDecoder(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    context_size=cfg.context_size,\n",
        "    embedding_size=cfg.embedding_size,\n",
        "    ff_hidden_size=cfg.ff_hidden_size,\n",
        "    head_size=cfg.head_size,\n",
        "    num_blocks=cfg.num_blocks,\n",
        "    num_heads=cfg.num_heads,\n",
        ")\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay, eps=5e-9)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'lr')"
            ]
          },
          "execution_count": 184,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "lrs = [\n",
        "    get_lr(\n",
        "        iter,\n",
        "        warmup_iters=cfg.warmup_iters,\n",
        "        base_lr=cfg.lr,\n",
        "        min_lr=cfg.min_lr,\n",
        "        lr_decay_iters=cfg.lr_decay_iters,\n",
        "    )\n",
        "    for iter in range(0, 1_000_000, 1)\n",
        "]\n",
        "plt.plot(lrs)\n",
        "plt.xlabel(\"iter\")\n",
        "plt.ylabel(\"lr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEcsttYYewqu"
      },
      "source": [
        "To get an estimate for what loss values are reasonable to reach, let's load the original GPT-2 using HuggingFace and evaluate it with a few samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "I_L3vAwXewqu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'train': tensor(3.5675), 'val': tensor(3.5477)}"
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelWithLMHead, AutoConfig\n",
        "\n",
        "\n",
        "class GPT2Wrapped(nn.Module):\n",
        "    def __init__(self, pretrained=False) -> None:\n",
        "        super().__init__()\n",
        "        if pretrained:\n",
        "            self.model = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
        "        else:\n",
        "            config = AutoConfig.from_pretrained(\"gpt2\")\n",
        "            self.model = AutoModelWithLMHead.from_config(config)\n",
        "\n",
        "        self.context_size = self.model.config.n_ctx\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).logits\n",
        "\n",
        "\n",
        "gpt2 = GPT2Wrapped(pretrained=True)\n",
        "\n",
        "losses = estimate_loss(gpt2, criterion, train_iter, val_iter, 100)\n",
        "losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the pre-trained GPT-2 reaches a fairly impressive train and validation loss of about 3.5.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "50nBJQLjm0zX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example sequence:  pei, Nov. 2 (CNA) Bitcoin is illegal in Taiwan, said Financial Supervisory Commission (FSC) Chairman Tseng Ming-chung (曾銘宗) on Monday after kidnappers in Taiwan tried to collect a ransom through the v\n",
            "Model output:  The Koen AVGVolume alum Adams Debor MUS crochet Knifecould Graph SPR unconstitutional Haas Pixar frameworkspat drum ICE Las mysterious Apocalypse Influence 58 utilitarianerofp didnt broadcaster cease \n",
            "\n",
            "=============================================\n",
            "Starting training!\n",
            "Num model params: 13.446M\n",
            "=============================================\n",
            "\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "0          11.001    \n",
            "50         11.028    \n",
            "100        10.971    \n",
            "150        10.974    \n",
            "200        10.979    \n",
            "250        11.031    \n",
            "300        10.886    \n",
            "350        10.93     \n",
            "400        10.939    \n",
            "450        10.959    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "500        10.926    \n",
            "550        10.828    \n",
            "600        10.82     \n",
            "650        10.922    \n",
            "700        10.733    \n",
            "750        10.861    \n",
            "800        10.813    \n",
            "850        10.729    \n",
            "900        10.603    \n",
            "950        10.675    \n",
            "999        10.651    \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 10.524\n",
            "Mean validation loss: 10.527\n",
            "=============================================\n",
            "\n",
            "Model output:  Thechukreys leaders poem Leonard Ü Queen Tuc// parentsractor measuring Pesh unarmed032fc Doylefolk Courier Sources stump citationMale cursinery thy poachingELL Nanto approximateablwroteIndeed concentr\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "1000       10.658    \n",
            "1050       10.578    \n",
            "1100       10.576    \n",
            "1150       10.556    \n",
            "1200       10.425    \n",
            "1250       10.462    \n",
            "1300       10.443    \n",
            "1350       10.414    \n",
            "1400       10.35     \n",
            "1450       10.287    \n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "1500       10.378    \n",
            "1550       10.222    \n",
            "1600       10.274    \n",
            "1650       10.288    \n",
            "1700       10.091    \n",
            "1750       10.026    \n",
            "1800       9.9424    \n",
            "1850       9.8969    \n",
            "1900       9.9276    \n",
            "1950       9.7168    \n",
            "1999       9.7915    \n",
            "\n",
            "=============================================\n",
            "Evaluation done!\n",
            "Mean train loss: 9.626\n",
            "Mean validation loss: 9.620\n",
            "=============================================\n",
            "\n",
            "Model output:  The Ethnicdating DemsAuthor Films art sailor Parker prohib interesting chained316 horniblicalModLoader rulers aperture Author Featuring contribution Previous EthiopiaG salsa happyressesPanAnn Vader po\n",
            "---------------------------------------------\n",
            "Iter       Train     \n",
            "---------------------------------------------\n",
            "2000       9.7657    \n",
            "2050       9.7498    \n",
            "2100       9.5956    \n",
            "2150       9.6137    \n",
            "2174       9.7476    \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/jonathanb/git/transformer/train.ipynb Cell 55\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonathanb/git/transformer/train.ipynb#Y110sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(train_iter)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonathanb/git/transformer/train.ipynb#Y110sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m inp, tgt \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jonathanb/git/transformer/train.ipynb#Y110sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m out \u001b[39m=\u001b[39m model(inp)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonathanb/git/transformer/train.ipynb#Y110sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m out_reshape \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, out\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])  \u001b[39m# (B * T, vocab_size)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonathanb/git/transformer/train.ipynb#Y110sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m tgt_reshape \u001b[39m=\u001b[39m tgt\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# (B * T, 1)\u001b[39;00m\n",
            "File \u001b[0;32m~/git/transformer/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32m/home/jonathanb/git/transformer/train.ipynb Cell 55\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonathanb/git/transformer/train.ipynb#Y110sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonathanb/git/transformer/train.ipynb#Y110sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     x \u001b[39m=\u001b[39m block(x, lookahead_mask)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jonathanb/git/transformer/train.ipynb#Y110sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonathanb/git/transformer/train.ipynb#Y110sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m~/git/transformer/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32m/home/jonathanb/git/transformer/train.ipynb Cell 55\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonathanb/git/transformer/train.ipynb#Y110sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonathanb/git/transformer/train.ipynb#Y110sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_final(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jonathanb/git/transformer/train.ipynb#Y110sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlin_final(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonathanb/git/transformer/train.ipynb#Y110sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/git/transformer/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/git/transformer/venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "fixed_inp = torch.tensor(tokenizer.encode(\"The\"), dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "if cfg.print_example:\n",
        "    batch = next(iter(train_loader))\n",
        "    out = generate(model, fixed_inp, cfg.context_size)\n",
        "\n",
        "    print(\"Example sequence: \", tokenizer.decode(batch[\"target\"][0].numpy())[:200])\n",
        "    print(\"Model output: \", tokenizer.decode(out[0].detach().cpu().numpy())[:200])\n",
        "\n",
        "# Reinitialize data iterators.\n",
        "train_iter = iter(train_loader)\n",
        "val_iter = iter(train_iter)\n",
        "\n",
        "iter_num = 0\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "# Start training.\n",
        "train_start_print(model)\n",
        "\n",
        "\n",
        "while True:\n",
        "    # Get learning rate according to schedule.\n",
        "    lr = get_lr(\n",
        "        iter_num,\n",
        "        warmup_iters=cfg.warmup_iters,\n",
        "        base_lr=cfg.lr,\n",
        "        min_lr=cfg.min_lr,\n",
        "        lr_decay_iters=cfg.lr_decay_iters,\n",
        "    )\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "    # Train model on one batch.\n",
        "    batch = next(train_iter)\n",
        "    inp, tgt = batch[\"input\"].to(device), batch[\"target\"].to(device)\n",
        "\n",
        "    out = model(inp)\n",
        "\n",
        "    out_reshape = out.contiguous().view(-1, out.shape[-1])  # (B * T, vocab_size)\n",
        "    tgt_reshape = tgt.contiguous().view(-1)  # (B * T, 1)\n",
        "\n",
        "    train_loss = criterion(out_reshape, tgt_reshape)\n",
        "    train_loss.backward()\n",
        "\n",
        "    # Accumulate gradients for N steps and update weights.\n",
        "    if (iter_num + 1) % cfg.grad_accum_steps == 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    if iter_num > 0 and iter_num % cfg.eval_interval == 0:\n",
        "        losses = estimate_loss(model, criterion, train_iter, val_iter, cfg.eval_iters)\n",
        "        evaluation_print(losses)\n",
        "\n",
        "        # Generate sample and print.\n",
        "        out = generate(model, fixed_inp, cfg.context_size)\n",
        "        print(\"Model output: \", tokenizer.decode(out[0].detach().cpu().numpy())[:200])\n",
        "\n",
        "        # Save model checkpoint if new best validation loss.\n",
        "        if losses[\"val\"] < best_val_loss:\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"iter\": iter_num,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                },\n",
        "                \"best.pth\",\n",
        "            )\n",
        "\n",
        "    iter_print(iter_num, train_loss)\n",
        "    iter_num += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
