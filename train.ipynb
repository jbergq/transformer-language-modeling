{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZflJMHzqm0zT"
      },
      "source": [
        "If you're opening this Notebook on colab, you will need to clone the repo and change directory. Uncomment the cell below and run it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqGXpJ6Vm0zV"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/jbergq/transformer.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoJqRYgypLE7"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "if Path.cwd().name != \"transformer\":\n",
        "  %cd transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1msvJlmNm0zW"
      },
      "outputs": [],
      "source": [
        "%pip install portalocker\n",
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from easydict import EasyDict\n",
        "\n",
        "cfg = EasyDict(\n",
        "    {\n",
        "        \"num_epochs\": 100,\n",
        "        \"batch_size\": 4,\n",
        "        \"lr\": 1e-3,\n",
        "        \"weight_decay\": 0.0005,\n",
        "        \"print_example\": True,\n",
        "    }\n",
        ")\n",
        "\n",
        "models = {\n",
        "    \"toy-model\": {\n",
        "        \"hidden_size\": 128,\n",
        "        \"ff_hidden_size\": 256,\n",
        "        \"num_blocks\": 4,\n",
        "        \"num_heads\": 4,\n",
        "        \"context_size\": 64,\n",
        "    },\n",
        "    \"gpt2-small\": {\n",
        "        \"hidden_size\": 768,\n",
        "        \"ff_hidden_size\": 3072,\n",
        "        \"num_blocks\": 12,\n",
        "        \"num_heads\": 12,\n",
        "        \"context_size\": 1024,\n",
        "    },\n",
        "}\n",
        "\n",
        "cfg.update(models[\"gpt2-small\"])\n",
        "\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVpo2t6O3QHQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "wandb.init(project=\"transformer\", config=cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_-7Ahq7m0zW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from src.model.transformer import TransformerDecoder\n",
        "from src.utils import iter_print, epoch_print\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's setup our dataset. We will use Hugging Face's `datasets` package to prepare and load the WebText dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"openwebtext\", streaming=True)\n",
        "# TODO: Load val/test set used in GPT-2 paper.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To tokenize our dataset, we will use the GPT-2 tokenizer, available from Hugging Face's `transformers` package.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Tokenizer used by GPT-2.\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# tokenizer.get_vocab()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(example):\n",
        "    outputs = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,  # Truncate returned token sequences to max_lenght.\n",
        "        max_length=cfg.context_size + 1,  # Max length of return token sequences.\n",
        "        return_overflowing_tokens=True,  # Tokenize whole input and split into chunks.\n",
        "        return_length=True,  # Return lengths of chunks.\n",
        "    )\n",
        "\n",
        "    source_batch = []\n",
        "    target_batch = []\n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "        if length == cfg.context_size + 1:\n",
        "            source_batch.append(input_ids[:-1])\n",
        "            target_batch.append(input_ids[1:])\n",
        "\n",
        "    return {\"source\": source_batch, \"target\": target_batch}\n",
        "\n",
        "\n",
        "dataset_train = dataset[\"train\"]\n",
        "dataset_train = dataset_train.map(\n",
        "    tokenize, batched=True, remove_columns=dataset_train.column_names\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=cfg.batch_size,\n",
        "    collate_fn=lambda samples: {\n",
        "        \"source\": torch.tensor([sample[\"source\"] for sample in samples]),\n",
        "        \"target\": torch.tensor([sample[\"target\"] for sample in samples]),\n",
        "    },\n",
        ")\n",
        "\n",
        "# Let's load a batch to confirm that it's working.\n",
        "batch = next(iter(train_dataloader))\n",
        "\n",
        "print(batch[\"source\"][0][:10])\n",
        "print(batch[\"target\"][0][:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bFIHBBDm0zX"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion, epoch):\n",
        "    train_losses = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        src, tgt = batch[\"source\"].to(device), batch[\"target\"].to(device)\n",
        "\n",
        "        out = model(src)\n",
        "\n",
        "        out_reshape = out.contiguous().view(-1, out.shape[-1])  # (B * T, vocab_size)\n",
        "        tgt_reshape = tgt.contiguous().view(-1)  # (B * T, 1)\n",
        "\n",
        "        loss = criterion(out_reshape, tgt_reshape)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_val = loss.item()\n",
        "        train_losses.append(loss_val)\n",
        "        iter_print(epoch, i, loss_val)\n",
        "\n",
        "    return torch.tensor(train_losses)\n",
        "\n",
        "\n",
        "def validate(model, dataloader, criterion, epoch):\n",
        "    val_losses = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        src, tgt = batch[\"source\"].to(device), batch[\"target\"].to(device)\n",
        "\n",
        "        out = model(src)\n",
        "\n",
        "        out_reshape = out.contiguous().view(-1, out.shape[-1])\n",
        "        tgt_reshape = tgt.contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(out_reshape, tgt_reshape)\n",
        "\n",
        "        loss_val = loss.item()\n",
        "        val_losses.append(loss_val)\n",
        "        iter_print(epoch, i, loss_val)\n",
        "\n",
        "        pred = out.softmax(dim=2).argmax(dim=2)\n",
        "\n",
        "    return torch.tensor(val_losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50nBJQLjm0zX"
      },
      "outputs": [],
      "source": [
        "model = TransformerDecoder(\n",
        "    tokenizer.vocab_size,\n",
        "    cfg.context_size,\n",
        "    cfg.hidden_size,\n",
        "    cfg.ff_hidden_size,\n",
        "    cfg.num_blocks,\n",
        "    cfg.num_heads,\n",
        ")\n",
        "model = model.to(device)\n",
        "optimizer = Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay, eps=5e-9)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
        "\n",
        "fixed_inp = torch.tensor(\n",
        "    tokenizer.encode(\"The\"), dtype=torch.long, device=device\n",
        ").unsqueeze(0)\n",
        "\n",
        "if cfg.print_example:\n",
        "    batch = next(iter(train_dataloader))\n",
        "    out = model.generate(fixed_inp)\n",
        "\n",
        "    print(\"Example sequence: \", tokenizer.decode(batch[\"target\"][0].numpy()))\n",
        "    print(\"Model output: \", tokenizer.decode(out[0].detach().cpu().numpy()))\n",
        "\n",
        "for epoch in range(cfg.num_epochs):\n",
        "    train_losses = train(model, train_dataloader, optimizer, criterion, epoch)\n",
        "    # val_losses = validate(model, val_dataloader, criterion, epoch)\n",
        "\n",
        "    wandb.log(\n",
        "        {\n",
        "            \"train_loss\": train_losses.mean().item(),\n",
        "            # \"val_loss\": val_losses.mean().item()\n",
        "        }\n",
        "    )\n",
        "    # epoch_print(epoch, val_losses)\n",
        "\n",
        "    out = model.generate(fixed_inp)\n",
        "    print(tokenizer.decode(out[0].detach().cpu().numpy()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
