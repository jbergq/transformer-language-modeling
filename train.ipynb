{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZflJMHzqm0zT"
      },
      "source": [
        "If you're opening this Notebook on colab, you will need to clone the repo and change directory. Uncomment the cell below and run it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqGXpJ6Vm0zV"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/jbergq/transformer.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoJqRYgypLE7"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "if Path.cwd().name != \"transformer\":\n",
        "  %cd transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1msvJlmNm0zW"
      },
      "outputs": [],
      "source": [
        "%pip install portalocker\n",
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from easydict import EasyDict\n",
        "\n",
        "\n",
        "# Define base config. Partly adopted from nanoGPT by Andrej Karpathy\n",
        "cfg = EasyDict(\n",
        "    {\n",
        "        \"val_size\": 1000,  # Size of validation set.\n",
        "        \"max_iters\": 600000,  # Total num training iterations.\n",
        "        \"eval_iters\": 100,  # Number of evaluation iterations.\n",
        "        \"eval_interval\": 1000,\n",
        "        \"effective_batch_size\": 512,\n",
        "        \"batch_size\": 4,\n",
        "        \"grad_accum_steps\": 1,\n",
        "        \"lr\": 1e-3,\n",
        "        \"warmup_iters\": 2000,\n",
        "        \"lr_decay_iters\": 600000,  # Should be ~= max_iters per Chinchilla.\n",
        "        \"min_lr\": 6e-5,  # Minimum learning rate, should be ~= learning_rate/10 per Chinchilla.\n",
        "        \"weight_decay\": 0.0005,\n",
        "        \"print_example\": True,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Define model configs.\n",
        "models = {\n",
        "    \"toy-model\": {\n",
        "        \"hidden_size\": 128,\n",
        "        \"ff_hidden_size\": 256,\n",
        "        \"num_blocks\": 4,\n",
        "        \"num_heads\": 4,\n",
        "        \"context_size\": 64,\n",
        "    },\n",
        "    \"gpt2-small\": {\n",
        "        \"hidden_size\": 768,\n",
        "        \"ff_hidden_size\": 3072,\n",
        "        \"num_blocks\": 12,\n",
        "        \"num_heads\": 12,\n",
        "        \"context_size\": 1024,\n",
        "    },\n",
        "}\n",
        "\n",
        "# Edit below to select a model.\n",
        "cfg.update(models[\"gpt2-small\"])\n",
        "\n",
        "# Derive accumulation steps to get target effective batch size.\n",
        "if cfg.effective_batch_size is not None:\n",
        "    cfg[\"grad_accum_steps\"] = cfg[\"effective_batch_size\"] // cfg[\"batch_size\"]\n",
        "\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVpo2t6O3QHQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "wandb.init(project=\"transformer\", config=cfg)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's setup our dataset. We will use Hugging Face's `datasets` package to prepare and load the WebText dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load WebText dataset in streaming mode. No need to download!\n",
        "dataset = load_dataset(\"openwebtext\", streaming=True)[\"train\"]\n",
        "shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10000)\n",
        "\n",
        "# Split dataset.\n",
        "train_set = shuffled_dataset.skip(cfg.val_size)\n",
        "val_set = shuffled_dataset.take(cfg.val_size)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To tokenize our dataset, we will use the GPT-2 tokenizer, available from Hugging Face's `transformers` package.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Tokenizer used by GPT-2.\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "\n",
        "def tokenize(example):\n",
        "    outputs = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,  # Truncate returned token sequences to max_length.\n",
        "        max_length=cfg.context_size + 1,  # Max length of returned token sequences.\n",
        "        return_overflowing_tokens=True,  # Tokenize whole input and split into chunks.\n",
        "        return_length=True,  # Return lengths of chunks.\n",
        "    )\n",
        "\n",
        "    # Create examples.\n",
        "    source_batch = []\n",
        "    target_batch = []\n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "        if length == cfg.context_size + 1:  # Only include full length sequences.\n",
        "            source_batch.append(input_ids[:-1])\n",
        "            target_batch.append(input_ids[1:])  # Note: Target is source shifted by one.\n",
        "\n",
        "    return {\"source\": source_batch, \"target\": target_batch}\n",
        "\n",
        "\n",
        "# Tokenize train and val sets.\n",
        "train_tokenized = train_set.map(\n",
        "    partial(tokenize),\n",
        "    batched=True,\n",
        "    remove_columns=train_set.column_names,\n",
        ")\n",
        "val_tokenized = val_set.map(\n",
        "    partial(tokenize),\n",
        "    batched=True,\n",
        "    remove_columns=val_set.column_names,\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training uses an \"infinite loop\" style, where we continue to sample random batches until we reach convergence or the maximum number of batches configured.\n",
        "\n",
        "Let's define a dataset wrapper that will allow us to continue sampling the dataset endlessly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Iterator\n",
        "\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "\n",
        "class InfiniteIterableDataset(IterableDataset):\n",
        "    def __init__(self, hf_dataset, shuffle=False):\n",
        "        self.hf_dataset = hf_dataset\n",
        "\n",
        "    def __iter__(self) -> Iterator:\n",
        "        while True:\n",
        "            for item in self.hf_dataset:\n",
        "                yield item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create data loaders for sampling batches.\n",
        "train_loader = DataLoader(\n",
        "    InfiniteIterableDataset(train_tokenized),\n",
        "    batch_size=cfg.batch_size,\n",
        "    collate_fn=lambda samples: {\n",
        "        \"source\": torch.tensor([sample[\"source\"] for sample in samples]),\n",
        "        \"target\": torch.tensor([sample[\"target\"] for sample in samples]),\n",
        "    },\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    InfiniteIterableDataset(val_tokenized),\n",
        "    batch_size=cfg.batch_size,\n",
        "    collate_fn=lambda samples: {\n",
        "        \"source\": torch.tensor([sample[\"source\"] for sample in samples]),\n",
        "        \"target\": torch.tensor([sample[\"target\"] for sample in samples]),\n",
        "    },\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load one train batch and one validation batch to make sure everything works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_train = next(iter(train_loader))\n",
        "\n",
        "print(batch_train[\"source\"][0][:10])\n",
        "print(batch_train[\"target\"][0][:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_val = next(iter(val_loader))\n",
        "\n",
        "print(batch_val[\"source\"][0][:10])\n",
        "print(batch_val[\"target\"][0][:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bFIHBBDm0zX"
      },
      "outputs": [],
      "source": [
        "def step(model, criterion, iterator):\n",
        "    batch = next(iterator)\n",
        "    src, tgt = batch[\"source\"].to(device), batch[\"target\"].to(device)\n",
        "\n",
        "    out = model(src)\n",
        "    # pred = out.softmax(dim=2).argmax(dim=2)\n",
        "\n",
        "    out_reshape = out.contiguous().view(-1, out.shape[-1])  # (B * T, vocab_size)\n",
        "    tgt_reshape = tgt.contiguous().view(-1)  # (B * T, 1)\n",
        "\n",
        "    loss = criterion(out_reshape, tgt_reshape)\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss estimation function inspired by nanoGPT repo by Andrej Karpathy.\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, criterion, train_iter, val_iter, eval_iters):\n",
        "    iterators = {\"train\": train_iter, \"val\": val_iter}\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split, iterator in iterators.items():\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            loss = step(model, criterion, iterator)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Learning rate decay scheduler inspired by nanoGPT repo by Andrej Karpathy.\n",
        "def get_lr(iter, warmup_iters, base_lr, min_lr, lr_decay_iters):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if iter < warmup_iters:\n",
        "        return base_lr * iter / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if iter > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (iter - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
        "    return min_lr + coeff * (base_lr - min_lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50nBJQLjm0zX"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "from src.model.transformer import TransformerDecoder\n",
        "from src.utils import train_start_print, iter_print, evaluation_print\n",
        "\n",
        "model = TransformerDecoder(\n",
        "    tokenizer.vocab_size,\n",
        "    cfg.context_size,\n",
        "    cfg.hidden_size,\n",
        "    cfg.ff_hidden_size,\n",
        "    cfg.num_blocks,\n",
        "    cfg.num_heads,\n",
        ")\n",
        "model = model.to(device)\n",
        "optimizer = Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay, eps=5e-9)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
        "\n",
        "fixed_inp = torch.tensor(\n",
        "    tokenizer.encode(\"The\"), dtype=torch.long, device=device\n",
        ").unsqueeze(0)\n",
        "\n",
        "if cfg.print_example:\n",
        "    batch = next(iter(train_loader))\n",
        "    out = model.generate(fixed_inp)\n",
        "\n",
        "    print(\"Example sequence: \", tokenizer.decode(batch[\"target\"][0].numpy())[:200])\n",
        "    print(\"Model output: \", tokenizer.decode(out[0].detach().cpu().numpy())[:200])\n",
        "\n",
        "iter_num = 0\n",
        "model.train()\n",
        "\n",
        "# Create data iterators.\n",
        "train_iter = iter(train_loader)\n",
        "val_iter = iter(train_iter)\n",
        "\n",
        "# Start training.\n",
        "train_start_print(model)\n",
        "while True:\n",
        "    # Get learning rate according to schedule.\n",
        "    lr = get_lr(iter_num, cfg.warmup_iters, cfg.lr, cfg.min_lr, cfg.lr_decay_iters)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "    # Train model on one batch.\n",
        "    train_loss = step(model, criterion, train_iter)\n",
        "    train_loss.backward()\n",
        "\n",
        "    # Accumulate gradients for N steps and update weights.\n",
        "    if (iter_num + 1) % cfg.grad_accum_steps == 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    if iter_num > 0 and iter_num % cfg.eval_interval == 0:\n",
        "        losses = estimate_loss(model, criterion, train_iter, val_iter, cfg.eval_iters)\n",
        "        evaluation_print(losses)\n",
        "\n",
        "        # Generate sample and print.\n",
        "        out = model.generate(fixed_inp)\n",
        "        print(\"Model output: \", tokenizer.decode(out[0].detach().cpu().numpy())[:200])\n",
        "\n",
        "        wandb.log(\n",
        "            {\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses[\"train\"],\n",
        "                \"val/loss\": losses[\"val\"],\n",
        "                \"lr\": lr,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    iter_print(iter_num, train_loss)\n",
        "    iter_num += 1\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
