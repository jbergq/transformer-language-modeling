# transformer

Yet another implementation of the Transformer model from the [Attention Is All You Need](https://arxiv.org/abs/1706.03762?context=cs) paper.

## Tasks

Goal is to complete the following:

- [x] Transformer model
- [x] Training pipeline
- [x] Load WebText dataset
- [ ] Train model and verify convergence
- [ ] Reproduce GPT-2 small results on LAMBADA dataset

## Usage

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jbergq/transformer/blob/main/train.ipynb)

## Other

For details around LAMBADA evaluation, see this [thread](https://github.com/huggingface/transformers/issues/491)
